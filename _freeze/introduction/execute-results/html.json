{
  "hash": "67e0f4021d7936fe7a7c8148e7a3d62e",
  "result": {
    "markdown": "# Introduction\n\n## Bayes theorem\n\nDenote by $p(X) \\equiv\\Pr(X)$ denotes the marginal density of $X$, $p(X \\mid Y)$ the conditional of $X$ given $Y$ and $p(X, Y)$ the joint density. Bayes' theorem states that\n\\begin{align*}\np(X = x \\mid Y=y) = \\frac{p(Y = y \\mid X = x)p(X=x)}{p(Y=y)}\n\\end{align*}\n\nIn the case of discrete random variable $X$ with support $\\mathcal{X},$ the denominator can be evaluated using the law of total probability  as $$\\Pr(Y=y) = \\sum_{x \\in \\mathcal{X}}\\Pr(Y =y \\mid X=x)\\Pr(X=x).$$\n\n::: {#exm-covidrapidtest}\n\nBack in January 2021, the Quebec government was debating whether or not to distribute antigen rapid test, with [strong reluctance](https://www.cbc.ca/news/canada/montreal/quebec-avoids-relying-on-rapid-covid-19-tests-as-pressure-mounts-to-follow-ontario-s-lead-1.5896738) from authorities given the paucity of available resources and the poor sensitivity.\n\nA Swiss study analyse the efficiency of rapid antigen tests, comparing them to repeated  polymerase chain reaction (PCR) test output, taken as benchmark [@Jegerlehner:2021]. The results are presented in @tbl-covid19\n\n|                | PCR $+$ |  PCR $-$ |\n|----------------|--------:|---------:|\n| rapid $+$      |      92 |        2 |\n| rapid $-$      |      49 |     1319 |\n| total          |     141 |     1321 |\n\n: Confusion matrix of Covid test results for PCR tests versus rapid antigen tests, from @Jegerlehner:2021. {#tbl-covid19}\n\n\nEstimated seropositivity at the end of January 2021 according to projections of the Institute for Health Metrics and Evaluation (IHME) of 8.18M out of 38M inhabitants [@owidcoronavirus], a prevalence of 21.4%. Assuming the latter holds uniformly over the country, what is the probability of having Covid if I get a negative result to a rapid test?\n\nLet $R^{-}$ ($R^{+}$) denote a negative (positive) rapid test result and $C^{+}$ ($C^{-}$) Covid positivity (negativity). Bayes' formula gives\n\\begin{align*}\n\\Pr(C^{+} \\mid R^{-}) & = \\frac{\\Pr(R^{-} \\mid C^{+})\\Pr(C^{+})}{\\Pr(R^{-} \\mid C^{+})\\Pr(C^{+}) + \\Pr(R^{-} \\mid C^{-})\\Pr(C^{-})} \\\\&=\n\\frac{49/141 \\cdot 0.214}{49/141 \\cdot 0.214 + 1319/1321 \\cdot 0.786}\n\\end{align*}\nso there is a small, but non-negligible probability of 8.66% that the rapid test result is misleading.  @Jegerlehner:2021 indeed found that the sensitivity was 65.3% among symptomatic individuals, but dropped down to 44% for asymptomatic cases. This may have fueled government experts skepticism.\n\n\n:::\n\n\n## Probability and frequency\n\nIn classical (frequentist) parametric statistic, we treat observations $\\boldsymbol{Y}$ as realizations of a distribution whose parameters $\\boldsymbol{\\theta}$ are unknown. All of the information about parameters is encoded by the likelihood function, which is optimized numerically or analytically to find the maximum likelihood estimator. Large-sample theory shows that the resulting estimator is asymptotically normal under regularity conditions.\n\nThe interpretation of probability in the classical statistic is understood in terms of long run frequency, which is why we call this approach frequentist statistic. Think of a fair die: when we state that values $\\{1, \\ldots, 6\\}$ are equiprobable, we mean that repeatedly tossing the die should result, in large sample, in each outcome being realized roughly $1/6$ of the time (the symmetry of the object also implies that each facet should be equally likely to lie face up). This interpretation also carries over to confidence intervals: a $(1-\\alpha)$ confidence interval either contains the true parameter value or it doesn't, so the probability level $(1-\\alpha)$ is only the long-run proportion of intervals created by the procedure that should contain the true fixed value, not the probability that a single interval contains the true value. This is counter-intuitive to most.\n\nIn practice, the true value of the parameter $\\boldsymbol{\\theta}$ vector is unknown to the practitioner, thus uncertain: Bayesians would argue that we should treat the latter as a random quantity rather than a fixed constant. Since different people may have different knowledge about these potential values, the prior knowledge is a form of subjective probability. For example, if you play cards, one person may have recorded the previous cards that were played, whereas other may not. They thus assign different probability of certain cards being played. In Bayesian inference, we consider $\\boldsymbol{\\theta}$ as random variables to reflect our lack of knowledge about potential values taken. Italian scientist Bruno de Finetti, who is famous for the claim ``Probability does not exist'', stated in the preface of @deFinetti:1974:\n\n> Probabilistic reasoning --- always to be understood as subjective --- merely stems from our being uncertain about something. It makes no difference whether the uncertainty relates to an unforseeable future, or to an unnoticed past, or to a past doubtfully reported or forgotten: it may even relate to something more or less knowable (by means of a computation, a logical deduction, etc.) but for which we are not willing or able tho make the effort; and so on [...]\n The only relevant thing is uncertainty --- the extent of our knowledge and ignorance. The actual fact of whether or not the events considered are in some sense *determined*, or known by other people, and so on, is of no consequence.\n\nOn page 3, de Finetti continues [@deFinetti:1974]\n\n> only subjective probabilities exist --- i.e., the degree of belief in the occurrence of an event attributed by a given person at a given instant and with a given set of information. \n\n\n\n\nThe likelihood $\\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) \\equiv p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})$ is the starting point for Bayesian inference. However, we adjoin to it a **prior** distribution $p(\\boldsymbol{\\theta})$ that reflects the prior knowledge about potential values taken by the $p$-dimensional parameter vector, before observing the data $\\boldsymbol{y}$. We thus seek $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$: the observations are random variables but inference is performed conditional on the observed sample. By Bayes' theorem, the posterior distribution $p(\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y})$ is\n\n$$\np(\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}) = \\frac{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\Theta}) p(\\boldsymbol{\\Theta})}{\\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}},\n$$ {#eq-posterior}\n\nso the posterior $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})$ is proportional, as a function of $\\theta$, to the product of the likelihood and the prior function. The integral in the denominator, termed marginal likelihood and denoted $p(\\boldsymbol{Y}) = \\mathsf{E}_{\\boldsymbol{\\theta}}\\{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})\\}$, is a normalizing constant that makes the right hand side integrate to unity. \n\nFor the posterior to be **proper**, we need the product on the right hand side to be integrable. The denominator of @eq-posterior is a normalizing constant so that the posterior is a distribution. If $\\boldsymbol{\\theta}$ is low dimensional, numerical integration such as quadrature methods can be used to compute the latter. To obtain the marginal posterior, $$p(\\theta_j \\mid \\boldsymbol{y}) = \\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}_{-j},$$ additional integration is needed. \n\n:::{exm-betabinomconjugate}\n\nConsider a binomial likelihood with probability of success $p$ and $n$ trials, $Y \\sim \\mathsf{Bin}(n, p)$. If we take a beta prior, $p \\sim \\mathsf{Be}(\\alpha, \\beta)$ and observe $k$ successes, the posterior is \n\\begin{align*}\np(\\theta \\mid y = k) &\\propto \\binom{n}{k} p^k (1-p)^{n-k} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}p^{\\alpha-1} (1-p)^{\\beta-1}\n\\\\&\\stackrel{p}{\\propto} p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}\n\\end{align*}\nand the normalizing constant is $$\\int_{0}^{1} p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}\\mathrm{d} p = \\frac{\\Gamma(k+\\alpha)\\Gamma(n-k+\\beta)}{\\Gamma(n+\\alpha+\\beta)},$$ a Beta function. Since we need only to keep track of the terms that are function of the parameter $p$, we could recognize directly that the posterior distribution is $\\mathsf{Be}(k+\\alpha, n-k+\\beta)$.\n\nIf the sample size $n$ grows, then the number of success should be roughly $np$ and the number of failures $n(1-p)$ and so the likelihood contribution, relative to the prior, will dominate. The Beta distribution, whose density is $f(x; \\alpha, \\beta) \\propto x^{\\alpha-1} (1-x)^{\\beta-1}$, has expectation $\\alpha/(\\alpha+\\beta)$ and variance $\\alpha\\beta/\\{(\\alpha+\\beta)^2(\\alpha+\\beta+1)\\}$. An alternative parametrization takes $\\alpha=\\mu \\kappa$, $\\beta = (1-\\mu)\\kappa$ for $\\mu \\in (0,1)$ and $\\kappa>0$, so that the model is parametrized directly in terms of mean $\\mu$.\n\n\n::: {.cell hash='introduction_cache/html/fig-betabinom_9b91bd128424b7e59d15dc9a9990b565'}\n::: {.cell-output-display}\n![Binomial likelihood for six successes out of 14 trials, $\\mathsf{Beta}(3/2, 3/2)$ prior and posterior distribution from a beta-binomial model. The posterior curve is much closer to the likelihood than it is to the prior, even with a relatively small sample size.](introduction_files/figure-html/fig-betabinom-1.png){#fig-betabinom width=672}\n:::\n:::\n\n\n:::\n\n\n\n::: {.keyidea name=\"The likelihood isn't a probability density function\"}\nWhile a density integrates to 1 over the range of possible outcomes, the likelihood function does not when we integrate over the range of the parameters.\n:::\n\n\n::: {.keyidea name=\"Improper priors\"}\n\nWe call a prior *proper* if it's integral is finite: the best example is priors that arise from probability density function. We can still employ this rule for improper priors: for example, taking $\\alpha, \\beta \\to 0$ in the Beta prior leads to a prior proportional to $x^{-1}(1-x)^{-1}$, the integral of which diverges on the unit interval $[0,1]$. However, as long as the number of success and the number of failures is larger than 1, meaning $k \\geq 1, n-k \\geq 1$, the posterior distribution would be proper, i.e., integrable. To find the posterior, normalizing constants are also superfluous.\n\n:::\n\n\n\n:::{exm-numericalintegration}\n\nThe beta-binomial model is an example of conjugate model, meaning the posterior distribution is from the same family as the prior.^[This is a property of exponential families that will be revisited in the next chapter.] While we could calculate analytically the value of the normalizing constant, we could also in more complicated models use numerical integration in the event the parameter vector $\\boldsymbol{\\theta}$ is low-dimensional. For a single scalar $p$ on the unit interval, numerical integration or Monte Carlo integration yield nearly identical results.\n\n\n::: {.cell hash='introduction_cache/html/unnamed-chunk-2_591e46c866647f736c805230405369f1'}\n\n```{.r .cell-code}\nk <- 6L # number of successes \nn <- 14L # number of trials\nalpha <- beta <- 1.5 # prior parameters\nunnormalized_posterior <- function(p){\n  p^(k+alpha-1) * (1-p)^(n-k + beta - 1)\n}\nintegrate(f = unnormalized_posterior,\n          lower = 0,\n          upper = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.066906e-05 with absolute error < 1e-12\n```\n:::\n\n```{.r .cell-code}\n# Compare with known constant\nbeta(k + alpha, n - k + beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.066906e-05\n```\n:::\n\n```{.r .cell-code}\n# Monte Carlo integration\nmean(unnormalized_posterior(runif(1e5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.064055e-05\n```\n:::\n:::\n\n\n\n:::\n\n\nWhen $\\boldsymbol{\\theta}$ is high-dimensional, the marginal likelihood is untractable. This is one of the main challenges of Bayesian statistics and the popularity and applicability has grown drastically with the development and popularity of numerical algorithms [@Geman.Geman:1984,@Gelfand.Smith:1990]. Markov chain Monte Carlo methods circumvent the calculation of the denominator by drawing approximate samples from the posterior. \n\n\n\n\n### Bayesian updating\n\n\nSubjective probabilities imply that different people with different prior beliefs would arrive at different conclusions. However, as more data are gathered, we can use Bayes theorem to update these prior beliefs and update the posterior. In most instances, the relative weight of the prior relative to the likelihood becomes negligible: if we consider independent data $\\boldsymbol{y}_1, \\boldsymbol{y}_n$ observed sequentially, then\n\\begin{align*}\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_k) &\\stackrel{\\boldsymbol{\\theta}}{\\propto} p(\\boldsymbol{y}_k \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_{k-1})\n\\\\ & \\stackrel{\\boldsymbol{\\theta}}{\\propto} \\prod_{i=1}^k p(\\boldsymbol{y}_i \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n\\end{align*}\nIf data are exchangeable, the order in which observations are collected and the order of the belief updating is irrelevant to the full posterior $p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n)$.\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}