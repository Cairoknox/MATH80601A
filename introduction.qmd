# Introduction

## Probability and frequency

In classical (frequentist) parametric statistic, we treat observations $\boldsymbol{Y}$ as realizations of a distribution whose parameters $\boldsymbol{\theta}$ are unknown. The *likelihood principle* states that all information about parameters is encoded by the likelihood function, which is optimized numerically or analytically to find the maximum likelihood estimator. This gives a single value for the parameter, and large-sample theory shows that the resulting estimator is asymptotically normal under regularity conditions.

The interpretation of probability in the classical statistic is somewhat counterintuitive and is understood in terms of long run frequency, which is why we call this approach frequentist statistic. Think of a fair die: when we state that values $\{1, \ldots, 6\}$ are equiprobable, what we mean is that repeatedly tossing the die should result, in large sample, in each outcome being realized roughly $1/6$ of the time (the symmetry of the object also implies they should equally likely). This interpretation also carries over to confidence intervals. A $(1-\alpha)$ confidence interval either contains the true parameter value or it doesn't, so the probability level is only the long-run proportion of intervals created by the procedure that should contain the true fixed value, not the probability that a single interval contains the true value. This is counterintuitive to most.

In practice, the true value of the parameter $\boldsymbol{\theta}$ vector is unknown to the practitionner, thus uncertain: Bayesians would argue that we should treat the latter as a random quantity rather than a fixed constant to reflect this lack of knowledge. Since different people may have different knowledge about these potential values, the prior knowledge is a form of subjective probabilities, meaning they are individual specific. For example, if you play cards, one person may have recorded the previous cards that were played, whereas other may not. They then assign different probability of certain cards being played. 


In Bayesian inference, we consider $\boldsymbol{\theta}$ as random variables to reflect our lack of knowledge about potential values taken. Italian scientist Bruno de Finetti, who is famous for the claim ``Probability does not exist'', stated in the preface of @deFinetti:1974:

> Probabilistic reasoning --- always to be understood as subjective --- merely stems from our being uncertain about something. It makes no difference whether the uncertainty relates to an unforseeable future, or to an unnoticed past, or to a past doubtfully reported or forgotten: it may even relate to something more or less knowable (by means of a computation, a logical deduction, etc.) but for which we are not willing or able tho make the effort; and so on [...]
 The only relevant thing is uncertainty --- the extent of our knowledge and ignorance. The actual fact of whether or not the events considered are in some sense *determined*, or known by other people, and so on, is of no consequence.

On page 3, de Finetti continues [@deFinetti:1974]

> only subjective probabilities exist --- i.e., the degree of belief in the occurrence of an event attributed by a given person at a given instant and with a given set of information. 




The likelihood $\mathcal{L}(\boldsymbol{\theta}; \boldsymbol{y}) \equiv p(\boldsymbol{y} \mid \boldsymbol{\theta})$ is the starting point for Bayesian inference. However, we adjoin to it a **prior** distribution $p(\boldsymbol{\theta})$ that reflects the prior knowledge about potential values taken by the $p$-dimensional parameter vector, before observing the data $\boldsymbol{y}$. We thus seek $p(\boldsymbol{\theta} \mid \boldsymbol{y})$: the observations are random variables but inference is performed conditional on the observed sample. By Bayes' theorem, the posterior distribution $p(\boldsymbol{\Theta} \mid \boldsymbol{Y})$ is

$$
p(\boldsymbol{\Theta} \mid \boldsymbol{Y}) = \frac{p(\boldsymbol{Y} \mid \boldsymbol{\Theta}) p(\boldsymbol{\Theta})}{\int p(\boldsymbol{Y} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d}\ \boldsymbol{\theta}},
$$ {#eq-posterior}

so the posterior $p(\boldsymbol{\theta} \mid \boldsymbol{y})$ is proportional, as a function of $\theta$, to the product of the likelihood and the prior function. The integral in the denominator, termed marginal likelihood and denoted $p(\boldsymbol{Y}) = \mathsf{E}_{\boldsymbol{\theta}}\{p(\boldsymbol{Y} \mid \boldsymbol{\theta})\}$, is a normalizing constant that makes the right hand side integrate to unity. 

For the posterior to be **proper**, we need the product on the right hand side to be integrable. The denominator of @eq-posterior is a normalizing constant so that the posterior is a distribution. If $\boldsymbol{\theta}$ is low dimensional, numerical integration such as quadrature methods can be used to compute the latter. To obtain the marginal posterior $p(\theta_j \mid \boldsymbol{y}) = \int p(\boldsymbol{\theta} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}_{-j}$, additional integration is needed. 

When $\boldsymbol{\theta}$ is high-dimensional, the marginal likelihood is untractable. This is one of the main challenges of Bayesian statistics and the popularity and applicability has grown drastically with the development and popularity of numerical algorithms [@Geman.Geman:1984,@Gelfand.Smith:1990]. Markov chain Monte Carlo methods circumvent the calculation of the denominator by drawing approximate samples from the posterior. 



### Bayesian updating


Subjective probabilities imply that different people with different prior beliefs would arrive at different conclusions. However, as more data are gathered, we can use Bayes theorem to update these prior beliefs and update the posterior. In most instances, the relative weight of the prior relative to the likelihood becomes negligible: if we consider independent data $\boldsymbol{y}_1, \boldsymbol{y}_n$ observed sequentially, then
\begin{align*}
p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \ldots, \boldsymbol{y}_k) &\stackrel{\boldsymbol{\theta}}{\propto} p(\boldsymbol{y}_k \mid \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \ldots, \boldsymbol{y}_{k-1})
\\ & \stackrel{\boldsymbol{\theta}}{\propto} \prod_{i=1}^k p(\boldsymbol{y}_i \mid \boldsymbol{\theta}) p(\boldsymbol{\theta})
\end{align*}
If data are exchangeable, the order in which observations are collected and the order of the belief updating is irrelevant to the full posterior $p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \ldots \boldsymbol{y}_n)$.









::: {#exm-covidrapidtest}

Back in January 2021, the Quebec government was debating whether or not to distribute antigen rapid test, with [strong reluctance](https://www.cbc.ca/news/canada/montreal/quebec-avoids-relying-on-rapid-covid-19-tests-as-pressure-mounts-to-follow-ontario-s-lead-1.5896738) from authorities given the paucity of available resources and the poor sensitivity.

A Swiss study analyse the efficiency of rapid antigen tests, comparing them to repeated  polymerase chain reaction (PCR) test output, taken as benchmark [@Jegerlehner:2021]. The results are presented in @tbl-covid19

|                | PCR $+$ |  PCR $-$ |
|----------------|--------:|---------:|
| rapid $+$      |      92 |        2 |
| rapid $-$      |      49 |     1319 |
| total          |     141 |     1321 |

: Confusion matrix of Covid test results for PCR tests versus rapid antigen tests, from @Jegerlehner:2021. {#tbl-covid19}


Estimated seropositivity at the end of January 2021 according to projections of the Institute for Health Metrics and Evaluation (IHME) of 8.18M out of 38M inhabitants [@owidcoronavirus], a prevalence of 21.4%. Assuming the latter holds uniformly over the country, what is the probability of having Covid if I get a negative result to a rapid test?

Let $\text{rapid} -$ ($\text{rapid} +$) denote a negative (positive) rapid test result and $\mathrm{C}+$ ($\mathrm{C}-$) Covid positivity (negativity). Bayes' formula gives
\begin{align*}
\Pr(\text{C}+ \mid \text{rapid} -) & = \frac{\Pr(\text{rapid} - \mid \text{C}+)\Pr(\text{C}+)}{\Pr(\text{rapid} - \mid \text{C}+)\Pr(\text{C}+) + \Pr(\text{rapid} - \mid \text{C}-)\Pr(\text{C}-)} \\&=
\frac{49/141 \cdot 0.214}{49/141 \cdot 0.214 + 1319/1321 \cdot 0.786} \\&\approx 0.0866
\end{align*}
so there is a small, but non-negligeable probability that the rapid test result is misleading.  @Jegerlehner:2021 indeed found that the sensitivity was 65.3% among symptomatic individuals, but dropped down to 44% for asymptomatic cases. This may have fueled government experts scepticism.


:::

