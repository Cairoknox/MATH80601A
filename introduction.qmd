# Introduction

## Bayes theorem

Denote by $p(X) \equiv\Pr(X)$ denotes the marginal density of $X$, $p(X \mid Y)$ the conditional of $X$ given $Y$ and $p(X, Y)$ the joint density. Bayes' theorem states that
\begin{align*}
p(X = x \mid Y=y) = \frac{p(Y = y \mid X = x)p(X=x)}{p(Y=y)}
\end{align*}

In the case of discrete random variable $X$ with support $\mathcal{X},$ the denominator can be evaluated using the law of total probability  as $$\Pr(Y=y) = \sum_{x \in \mathcal{X}}\Pr(Y =y \mid X=x)\Pr(X=x).$$

::: {#exm-covidrapidtest}

Back in January 2021, the Quebec government was debating whether or not to distribute antigen rapid test, with [strong reluctance](https://www.cbc.ca/news/canada/montreal/quebec-avoids-relying-on-rapid-covid-19-tests-as-pressure-mounts-to-follow-ontario-s-lead-1.5896738) from authorities given the paucity of available resources and the poor sensitivity.

A Swiss study analyse the efficiency of rapid antigen tests, comparing them to repeated  polymerase chain reaction (PCR) test output, taken as benchmark [@Jegerlehner:2021]. The results are presented in @tbl-covid19

|                | PCR $+$ |  PCR $-$ |
|----------------|--------:|---------:|
| rapid $+$      |      92 |        2 |
| rapid $-$      |      49 |     1319 |
| total          |     141 |     1321 |

: Confusion matrix of Covid test results for PCR tests versus rapid antigen tests, from @Jegerlehner:2021. {#tbl-covid19}


Estimated seropositivity at the end of January 2021 according to projections of the Institute for Health Metrics and Evaluation (IHME) of 8.18M out of 38M inhabitants [@owidcoronavirus], a prevalence of 21.4%. Assuming the latter holds uniformly over the country, what is the probability of having Covid if I get a negative result to a rapid test?

Let $R^{-}$ ($R^{+}$) denote a negative (positive) rapid test result and $C^{+}$ ($C^{-}$) Covid positivity (negativity). Bayes' formula gives
\begin{align*}
\Pr(C^{+} \mid R^{-}) & = \frac{\Pr(R^{-} \mid C^{+})\Pr(C^{+})}{\Pr(R^{-} \mid C^{+})\Pr(C^{+}) + \Pr(R^{-} \mid C^{-})\Pr(C^{-})} \\&=
\frac{49/141 \cdot 0.214}{49/141 \cdot 0.214 + 1319/1321 \cdot 0.786}
\end{align*}
so there is a small, but non-negligible probability of 8.66% that the rapid test result is misleading.  @Jegerlehner:2021 indeed found that the sensitivity was 65.3% among symptomatic individuals, but dropped down to 44% for asymptomatic cases. This may have fueled government experts skepticism.


:::


## Probability and frequency

In classical (frequentist) parametric statistic, we treat observations $\boldsymbol{Y}$ as realizations of a distribution whose parameters $\boldsymbol{\theta}$ are unknown. All of the information about parameters is encoded by the likelihood function, which is optimized numerically or analytically to find the maximum likelihood estimator. Large-sample theory shows that the resulting estimator is asymptotically normal under regularity conditions.

The interpretation of probability in the classical statistic is understood in terms of long run frequency, which is why we call this approach frequentist statistic. Think of a fair die: when we state that values $\{1, \ldots, 6\}$ are equiprobable, we mean that repeatedly tossing the die should result, in large sample, in each outcome being realized roughly $1/6$ of the time (the symmetry of the object also implies that each facet should be equally likely to lie face up). This interpretation also carries over to confidence intervals: a $(1-\alpha)$ confidence interval either contains the true parameter value or it doesn't, so the probability level $(1-\alpha)$ is only the long-run proportion of intervals created by the procedure that should contain the true fixed value, not the probability that a single interval contains the true value. This is counter-intuitive to most.

In practice, the true value of the parameter $\boldsymbol{\theta}$ vector is unknown to the practitioner, thus uncertain: Bayesians would argue that we should treat the latter as a random quantity rather than a fixed constant. Since different people may have different knowledge about these potential values, the prior knowledge is a form of **subjective probability**. For example, if you play cards, one person may have recorded the previous cards that were played, whereas other may not. They thus assign different probability of certain cards being played. In Bayesian inference, we consider $\boldsymbol{\theta}$ as random variables to reflect our lack of knowledge about potential values taken. Italian scientist Bruno de Finetti, who is famous for the claim ``Probability does not exist'', stated in the preface of @deFinetti:1974:

> Probabilistic reasoning --- always to be understood as subjective --- merely stems from our being uncertain about something. It makes no difference whether the uncertainty relates to an unforseeable future, or to an unnoticed past, or to a past doubtfully reported or forgotten: it may even relate to something more or less knowable (by means of a computation, a logical deduction, etc.) but for which we are not willing or able tho make the effort; and so on [...]
 The only relevant thing is uncertainty --- the extent of our knowledge and ignorance. The actual fact of whether or not the events considered are in some sense *determined*, or known by other people, and so on, is of no consequence.

On page 3, de Finetti continues [@deFinetti:1974]

> only subjective probabilities exist --- i.e., the degree of belief in the occurrence of an event attributed by a given person at a given instant and with a given set of information. 




The likelihood $\mathcal{L}(\boldsymbol{\theta}; \boldsymbol{y}) \equiv p(\boldsymbol{y} \mid \boldsymbol{\theta})$ is the starting point for Bayesian inference. However, we adjoin to it a **prior** distribution $p(\boldsymbol{\theta})$ that reflects the prior knowledge about potential values taken by the $p$-dimensional parameter vector, before observing the data $\boldsymbol{y}$. We thus seek $p(\boldsymbol{\theta} \mid \boldsymbol{y})$: the observations are random variables but inference is performed conditional on the observed sample. By Bayes' theorem, the posterior distribution $p(\boldsymbol{\Theta} \mid \boldsymbol{Y})$ is

$$
p(\boldsymbol{\Theta} \mid \boldsymbol{Y}) = \frac{p(\boldsymbol{Y} \mid \boldsymbol{\Theta}) p(\boldsymbol{\Theta})}{\int p(\boldsymbol{Y} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}},
$$ {#eq-posterior}

so the posterior $p(\boldsymbol{\theta} \mid \boldsymbol{y})$ is proportional, as a function of $\theta,$ to the product of the likelihood and the prior function. The integral in the denominator, termed marginal likelihood and denoted $p(\boldsymbol{Y}) = \mathsf{E}_{\boldsymbol{\theta}}\{p(\boldsymbol{Y} \mid \boldsymbol{\theta})\}$, is a normalizing constant that makes the right hand side integrate to unity. 

For the posterior to be **proper**, we need the product on the right hand side to be integrable. The denominator of @eq-posterior is a normalizing constant so that the posterior is a distribution. If $\boldsymbol{\theta}$ is low dimensional, numerical integration such as quadrature methods can be used to compute the latter. To obtain the marginal posterior, $$p(\theta_j \mid \boldsymbol{y}) = \int p(\boldsymbol{\theta} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}_{-j},$$ additional integration is needed. 

:::{exm-betabinomconjugate}

Consider a binomial likelihood with probability of success $p$ and $n$ trials, $Y \sim \mathsf{Bin}(n, p)$. If we take a beta prior, $p \sim \mathsf{Be}(\alpha, \beta)$ and observe $k$ successes, the posterior is 
\begin{align*}
p(\theta \mid y = k) &\propto \binom{n}{k} p^k (1-p)^{n-k} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha-1} (1-p)^{\beta-1}
\\&\stackrel{p}{\propto} p^{k+\alpha-1}(1-p)^{n-k+\beta-1}
\end{align*}
and is $$\int_{0}^{1} p^{k+\alpha-1}(1-p)^{n-k+\beta-1}\mathrm{d} p = \frac{\Gamma(k+\alpha)\Gamma(n-k+\beta)}{\Gamma(n+\alpha+\beta)},$$ a Beta function. Since we need only to keep track of the terms that are function of the parameter $p$, we could recognize directly that the posterior distribution is $\mathsf{Be}(k+\alpha, n-k+\beta)$ and deduce the normalizing constant from there.

The number of success should be roughly $np$ and the number of failures $n(1-p)$ and so the likelihood contribution, relative to the prior, will dominate as the sample size $n$ grows. 

Another way to see this is to track moments (expectation, variance, etc.)
The Beta distribution, whose density is $f(x; \alpha, \beta) \propto x^{\alpha-1} (1-x)^{\beta-1}$, has expectation $\alpha/(\alpha+\beta)$ and variance $\alpha\beta/\{(\alpha+\beta)^2(\alpha+\beta+1)\}$ and so as $n \to \infty$, the mean of the posterior will be dominated by information from the likelihood. An alternative parametrization takes $\alpha=\mu \kappa$, $\beta = (1-\mu)\kappa$ for $\mu \in (0,1)$ and $\kappa>0$, so that the model is parametrized directly in terms of mean $\mu$.

```{r}
#| label: fig-betabinom
#| eval: true
#| echo: false
#| fig-cap: "Binomial likelihood for six successes out of 14 trials, $\\mathsf{Beta}(3/2, 3/2)$ prior and posterior distribution from a beta-binomial model. The posterior curve is much closer to the likelihood than it is to the prior, even with a relatively small sample size."
library(ggplot2)
library(MetBrewer)
library(patchwork)

set.seed(1234)
n <- rpois(n = 1, lambda = 20)
p <- 0.4
k <- rbinom(n = 1, size = n, prob = p)
binlik <- function(p){
  dbinom(x = k, size = n, prob = p) * choose(n,k) * beta(k+1, n-k+1)
  }
alpha <- 1.5
beta <- 1.5

ggplot() +
    stat_function(fun =  function(x){
        dbeta(x, shape1 = k, shape2 = n - k)},
        mapping = aes(fill = "likelihood",
                      col = "likelihood"),
        alpha = 0.2,
        geom = "area") +
    stat_function(fun = function(x){
        dbeta(x, shape1 = alpha, shape2 = beta)}, 
        alpha = 0.2,
        group = factor(2),
        mapping = aes(fill = "prior",
                      col = "prior"),
        geom = "area") +
    stat_function(fun = function(x){
        dbeta(x, shape1 = k + alpha, shape2 = n - k + beta)},
        mapping = aes(fill = "posterior", 
                      col = "posterior"), 
        group = factor(3),
        alpha = 0.2,
        geom = "area") +
    scale_x_continuous(limits = c(0,1),
                       expand = c(0,0),
                       breaks = seq(0, 1, by = 0.25),
                       labels = c("0", "0.25", "0.5", "0.75", "1")) +
    scale_y_continuous(expand = c(0,0)) +
    scale_color_manual(values = met.brewer("Renoir", 3), 
                       labels = c("likelihood","posterior","prior"),
                       aesthetics = c("colour", "fill")) +
    labs(y = "",
         colour = "",
         fill = "") + 
    theme_classic()
```

:::



::: {.keyidea name="The likelihood isn't a probability density function"}
While a density integrates to 1 over the range of possible outcomes, the likelihood function does not when we integrate over the range of the parameters.
:::




:::{exm-numericalintegration}

The beta-binomial model is an example of conjugate model, meaning the posterior distribution is from the same family as the prior.^[This is a property of exponential families that will be revisited in the next chapter.] While we could calculate analytically the value of the normalizing constant, we could also for arbitrary priors use numerical integration in the event the parameter vector $\boldsymbol{\theta}$ is low-dimensional. For a single scalar $p$ on the unit interval, numerical integration or Monte Carlo integration yield nearly identical results.

```{r}
#| eval: true
#| echo: true
#| cache: true
k <- 6L # number of successes 
n <- 14L # number of trials
alpha <- beta <- 1.5 # prior parameters
unnormalized_posterior <- function(p){
  p^(k+alpha-1) * (1-p)^(n-k + beta - 1)
}
integrate(f = unnormalized_posterior,
          lower = 0,
          upper = 1)
# Compare with known constant
beta(k + alpha, n - k + beta)
# Monte Carlo integration
mean(unnormalized_posterior(runif(1e5)))
# Alternative approach, sampling from the prior
# This is less efficient
mean(dbinom(x = k, 
            size = n, 
            prob = rbeta(n = 1e6, alpha, beta))) *
  beta(alpha, beta) / choose(n, k)
```


:::


When $\boldsymbol{\theta}$ is high-dimensional, the marginal likelihood is untractable. This is one of the main challenges of Bayesian statistics and the popularity and applicability has grown drastically with the development and popularity of numerical algorithms, following th publication of @Geman.Geman:1984 and @Gelfand.Smith:1990. Markov chain Monte Carlo methods circumvent the calculation of the denominator by drawing approximate samples from the posterior. 




### Bayesian updating


Subjective probabilities imply that different people with different prior beliefs would arrive at different conclusions. However, as more data are gathered, we can use Bayes theorem to update these prior beliefs and update the posterior. In most instances, the relative weight of the prior relative to the likelihood becomes negligible: if we consider independent data $\boldsymbol{y}_1, \boldsymbol{y}_n$ observed sequentially, then
\begin{align*}
p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \ldots, \boldsymbol{y}_k) &\stackrel{\boldsymbol{\theta}}{\propto} p(\boldsymbol{y}_k \mid \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \ldots, \boldsymbol{y}_{k-1})
\\ & \stackrel{\boldsymbol{\theta}}{\propto} \prod_{i=1}^k p(\boldsymbol{y}_i \mid \boldsymbol{\theta}) p(\boldsymbol{\theta})
\end{align*}
If data are exchangeable, the order in which observations are collected and the order of the belief updating is irrelevant to the full posterior $p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \ldots \boldsymbol{y}_n)$.









