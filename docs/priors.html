<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian modelling - 2&nbsp; Priors</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./references.html" rel="next">
<link href="./introduction.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./priors.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/math80601a/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#conjugate-priors" id="toc-conjugate-priors" class="nav-link active" data-scroll-target="#conjugate-priors"><span class="header-section-number">2.1</span> Conjugate priors</a></li>
  <li><a href="#uninformative-priors" id="toc-uninformative-priors" class="nav-link" data-scroll-target="#uninformative-priors"><span class="header-section-number">2.2</span> Uninformative priors</a></li>
  <li><a href="#expert-knowledge" id="toc-expert-knowledge" class="nav-link" data-scroll-target="#expert-knowledge"><span class="header-section-number">2.3</span> Expert knowledge</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/lbelzile/math80601a/edit/main/priors.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></h1>
</header>

<p>The posterior distribution combines two ingredients: the likelihood and the prior. If the former is a standard ingredient of any likelihood-based inference, prior specification requires some care. The purpose of this chapter is to consider different standard way of constructing prior functions.</p>
<section id="conjugate-priors" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="conjugate-priors"><span class="header-section-number">2.1</span> Conjugate priors</h2>
<p>A distribution belongs to an exponential family with parameter vector <span class="math inline">\(\boldsymbol{\theta} \in \mathbb{R}^D\)</span> if it can be written as <span class="math display">\[\begin{align*}
f(y; \boldsymbol{\theta}) = \exp\left\{ \sum_{k=1}^K Q_k(\boldsymbol{\theta}) t_k(y) + D(\boldsymbol{\theta})\right\}
\end{align*}\]</span> and in particular, the support does not depend on unknown parameters. If we have an independent and identically distributed sample of observations <span class="math inline">\(y_1, \ldots, y_n\)</span>, the log likelihood is thus of the form <span class="math display">\[\begin{align*}
\ell(\boldsymbol{\theta}) = \sum_{k=1}^K \phi_k(\boldsymbol{\theta}) \sum_{i=1}^n t_k(y_i) + n D(\boldsymbol{\theta}),
\end{align*}\]</span> where the collection <span class="math inline">\(\sum_{i=1}^n t_k(y_i)\)</span> (<span class="math inline">\(k=1, \ldots, K\)</span>) are sufficient statistics and <span class="math inline">\(\phi_k(\boldsymbol{\theta})\)</span> are the canonical parameters. The number of sufficient statistics are the same regardless of the sample size. Exponential families play a prominent role in generalized linear models, in which the natural parameters are modelled as linear function of explanatories.</p>
<p>A log prior density that is proportional to <span class="math display">\[\begin{align*}
\log p(\boldsymbol{\theta}) \propto \eta D(\boldsymbol{\theta}) + \sum_{k=1}^K Q_k(\boldsymbol{\theta}) \nu_k
\end{align*}\]</span> is conjugate.</p>
<div id="exm-conjugatepriors-binom" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (Conjugate priors for the binomial model) </strong></span>The binomial log density with <span class="math inline">\(y\)</span> successes out of <span class="math inline">\(n\)</span> trials is proportional to <span class="math display">\[\begin{align*}
y \log(p) + (n-y) \log(1-p) = y\log\left( \frac{p}{1-p}\right) + n \log(1-p)
\end{align*}\]</span> with canonical parameter <span class="math inline">\(\mathrm{logit}(p)\)</span>, which is the natural link function for Bernoulli, giving rise to logistic regresion model.</p>
<p>Since the density of the binomial is of the form <span class="math inline">\(p^y(1-p)^{n-y}\)</span>, the beta distribution <span class="math inline">\(\mathsf{Be}(\alpha, \beta)\)</span> with density <span class="math inline">\(f(x) \propto x^{\alpha-1} (1-x)^{\beta-1}\)</span> is the conjugate prior.</p>
<p>The posterior mean <span class="math display">\[\begin{align*}
\mathsf{E}(p \mid y) = w\frac{y}{n} + (1-w) \frac{a}{a+b},
\qquad w = \frac{n}{n+a+b}
\end{align*}\]</span> is therefore a weighted average of the maximum likelihood estimator and the prior mean. We can think of the parameter <span class="math inline">\(\alpha\)</span> (respectively <span class="math inline">\(\beta\)</span>) as representing the prior number of success (resp. failures).</p>
</div>
<div id="exm-conjugatepriors-poisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.2 (Conjugate prior for the Poisson model) </strong></span>The Poisson distribution with mean <span class="math inline">\(\mu\)</span> has log density proportional to <span class="math inline">\(f(y; \mu) \propto y\log(\mu) -\mu\)</span>, so is an exponential family with natural parameter <span class="math inline">\(\log(\mu)\)</span>. The gamma distribution, <span class="math inline">\(p(x) \propto \beta^{\alpha}/\Gamma(\alpha)x^{\alpha-1} \exp(-\beta x)\)</span> with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span> is the conjugate prior for the Poisson. For an <span class="math inline">\(n\)</span>-sample of independent observations <span class="math inline">\(\mathsf{Po}(\mu)\)</span> observations with <span class="math inline">\(\mu \sim \mathsf{Ga}(\alpha, \beta)\)</span>, the posterior is <span class="math inline">\(\mathsf{Ga}(\sum_{i=1}^n y_i + \alpha, \beta + n)\)</span>.</p>
</div>
<div id="exm-abtest" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.3 (Posterior rates for A/B tests using conjugate Poisson model) </strong></span>Upworthy.com, a US media publisher, revolutionized headlines online advertisement by running systematic A/B tests to compare the different wording of headlines, placement and image and what catches attention the most. The Upworthy Research Archive <span class="citation" data-cites="Matias:2021">(<a href="references.html#ref-Matias:2021" role="doc-biblioref">Matias et al. 2021</a>)</span> contains results for 22743 experiments, with a click through rate of 1.58% on average and a standard deviation of 1.23%. The <code>clickability_test_id</code> gives the unique identifier of the experiment, <code>clicks</code> the number of conversion out of <code>impressions</code>. See <a href="https://tellingstorieswithdata.com/08-hunt.html#ab-testing">Section 8.5</a> of <span class="citation" data-cites="Alexander:2023">Alexander (<a href="references.html#ref-Alexander:2023" role="doc-biblioref">2023</a>)</span> for more details about A/B testing and background information.</p>
<p>Consider an A/B test from November 23st, 2014, that compared four different headlines for a story on Sesame Street workshop with interviews of children whose parents were in jail and visiting them in prisons. The headlines tested were:</p>
<blockquote class="blockquote">
<ol type="1">
<li>Some Don’t Like It When He Sees His Mom. But To Him? Pure Joy. Why Keep Her From Him?</li>
<li>They’re Not In Danger. They’re Right. See True Compassion From The Children Of The Incarcerated.</li>
<li>Kids Have No Place In Jail … But In This Case, They <em>Totally</em> Deserve It.</li>
<li>Going To Jail <em>Should</em> Be The Worst Part Of Their Life. It’s So Not. Not At All.</li>
</ol>
</blockquote>
<p>At first glance, the first and third headlines seem likely to lead to a curiosity gap. The wording of the second is more explicit (and searchable), whereas the first is worded as a question.</p>
<p>We model the conversion rate <span class="math inline">\(\lambda_i\)</span> for each headline separately using a Poisson distribution and compare the posterior distributions for all four choices. Using a conjugate prior and selecting the parameters by moment matching yields approximately <span class="math inline">\(\alpha = 1.64\)</span> and <span class="math inline">\(\beta = 0.01\)</span> for the hyperparameters.</p>
<div class="cell" data-hash="priors_cache/html/tbl-upworthy_3b712e8caa487ceac67eee58af719851">
<div class="cell-output-display">
<div id="tbl-upworthy" class="anchored">
<table class="table table-sm table-striped small">
<caption>Table&nbsp;2.1: Number of views, clicks for different headlines for the Upworthy data.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">headline</th>
<th style="text-align: right;">impressions</th>
<th style="text-align: right;">clicks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">H1</td>
<td style="text-align: right;">3060</td>
<td style="text-align: right;">49</td>
</tr>
<tr class="even">
<td style="text-align: left;">H2</td>
<td style="text-align: right;">2982</td>
<td style="text-align: right;">20</td>
</tr>
<tr class="odd">
<td style="text-align: left;">H3</td>
<td style="text-align: right;">3112</td>
<td style="text-align: right;">31</td>
</tr>
<tr class="even">
<td style="text-align: left;">H4</td>
<td style="text-align: right;">3083</td>
<td style="text-align: right;">9</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-hash="priors_cache/html/fig-upworthy_d1be2a0a38ceeb2b37fdea0bab89f42e">
<div class="cell-output-display">
<div id="fig-upworthy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="priors_files/figure-html/fig-upworthy-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;2.1: Gamma posterior for the Upworthy Sesame street headline.</figcaption>
</figure>
</div>
</div>
</div>
<p>We can visualize the posterior distributions. In this context, the large sample size lead to the dominance of the likelihood contribution <span class="math inline">\(p(Y_i \mid \lambda_i) \sim \mathsf{Po}(n_i\lambda_i)\)</span> relative to the prior. We can see there is virtually no overlap between different rates for headers H1 (preferred) relative to H4 (least favorable). The probability that Headline 3 is better than Headline 1 can be approximated by simulating samples from both posterior and computing the proportion of times one is larger: the probability of superiority is 1.7%, indicating a clear preference for the first headline <code>H1</code>.</p>
</div>
<div id="exm-conjugatepriors-normal" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.4 (Conjugate priors in the Bayesian linear model) </strong></span>Consider a linear regression model with observation-specific mean <span class="math inline">\(\mu_i = \mathbf{x}_i\boldsymbol{\beta}\)</span> <span class="math inline">\((i=1,\ldots, n)\)</span> with <span class="math inline">\(\mathbf{x}_i\)</span> the <span class="math inline">\(i\)</span>th row of the <span class="math inline">\(n \times p\)</span> design matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Concatenating records, <span class="math inline">\(\boldsymbol{Y} \sim \mathsf{No}_n(\mathbf{X}\boldsymbol{\beta}, \sigma^2 \mathbf{Q}_y^{-1})\)</span>, for a known precision matrix <span class="math inline">\(\mathbf{Q}_y\)</span>, typically <span class="math inline">\(\mathbf{I}_n\)</span>. To construct a conjugate joint prior for <span class="math inline">\(p(\boldsymbol{\beta}, \sigma^2)\)</span>, we consider the sequential formulation <span class="math display">\[\begin{align*}
\boldsymbol{\beta} \mid \sigma^2 \sim \mathsf{No}_p(\boldsymbol{\nu}_\beta, \sigma^2 \mathbf{Q}^{-1}_\beta), \qquad \sigma^2 \sim \mathsf{IG}(\alpha,\beta)
\end{align*}\]</span> where <span class="math inline">\(\mathsf{IG}\)</span> denotes the inverse gamma distribution<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<!--
Writing the log likelihood in exponential family form,
\begin{align*}
\ell(\mu, \sigma^2; \boldsymbol{y}) \propto -n \log(\sigma) - \frac{\boldsymbol{y}^\top\boldsymbol{y}}{2\sigma^2} + \frac{\mu}{\sigma^2}\boldsymbol{y}^\top \boldsymbol{1}_n - \frac{n}{2}\frac{\mu^2}{\sigma^2}
\end{align*}

-->
<p>The joint posterior is Gaussian-inverse gamma and can be factorized <span class="math display">\[\begin{align*}
p(\boldsymbol{\beta}, \sigma^2 \mid y) = p(\sigma^2 \mid y) p(\boldsymbol{\beta} \mid \sigma^2, y)
\end{align*}\]</span> where <span class="math inline">\(p(\sigma^2 \mid y) \sim \mathsf{IG}(\alpha^*, \beta^*)\)</span> and <span class="math inline">\(p(\boldsymbol{\beta} \mid \sigma^2, y) \sim \mathsf{No}_p(\mathbf{M}\boldsymbol{m}, \sigma^2\mathbf{M})\)</span> with <span class="math inline">\(\alpha^* = \alpha + n/2\)</span>, <span class="math inline">\(\beta^*=\beta + 0.5 \boldsymbol{\nu}_\beta^\top \mathbf{Q}_\beta\boldsymbol{\nu}_\beta + \boldsymbol{y}^\top\boldsymbol{y} - \boldsymbol{m}^\top\mathbf{M}\boldsymbol{m}\)</span>, <span class="math inline">\(\boldsymbol{m} = \mathbf{Q}_\beta \boldsymbol{\nu}_\beta + \mathbf{X}^\top \mathbf{Q}_y\boldsymbol{y}\)</span> and <span class="math inline">\(\mathbf{M} = (\mathbf{Q}_\beta + \mathbf{X}^\top\mathbf{Q}_y\mathbf{X})^{-1}\)</span>; the latter can be evaluated efficiently using Shermann–Morrisson–Woodbury identity.</p>
</div>
<p>The exponential family is quite large; <a href="https://doi.org/10.1.1.157.5540">Fink (1997) <em>A Compendium of Conjugate Priors</em></a> gives multiple examples of conjugate priors and work out parameter values.</p>
<div class="keyidea" name="Objective and subjective Bayes">
<p>One criticism of the Bayesian approach is the arbitrariness of prior functions. However, the role of the prior is often negligible in large samples (consider for example the posterior of exponential families with conjugate priors). Moreover, the likelihood is also chosen for convenience, and arguably has a bigger influence on the conclusion. Data fitted using a linear regression model seldom follow Gaussian distributions conditionally, in the same way that the linearity is a convenience (and first order approximation).</p>
</div>
<p>In general, unless the sample size is small and we want to add expert opinion, we may wish to pick an <em>uninformative prior</em>, i.e., one that does not impact much the outcome. For conjugate models, one can often show that the relative weight of prior parameters (relative to the random sample likelihood contribution) becomes negligible by <a href="https://en.wikipedia.org/wiki/Conjugate_prior">investigating their relative weights</a>.</p>
</section>
<section id="uninformative-priors" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="uninformative-priors"><span class="header-section-number">2.2</span> Uninformative priors</h2>
<div id="def-properprior" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (Proper prior) </strong></span>We call a prior <em>proper</em> if it’s integral is finite; such prior function automatically leads to a valid posterior.</p>
</div>
<p>The best example of prior priors arise from probability density function. We can still employ this rule for improper priors: for example, taking <span class="math inline">\(\alpha, \beta \to 0\)</span> in the beta prior leads to a prior proportional to <span class="math inline">\(x^{-1}(1-x)^{-1}\)</span>, the integral of which diverges on the unit interval <span class="math inline">\([0,1]\)</span>. However, as long as the number of success and the number of failures is larger than 1, meaning <span class="math inline">\(k \geq 1, n-k \geq 1\)</span>, the posterior distribution would be proper, i.e., integrable. To find the posterior, normalizing constants are also superfluous.</p>
<p>Many uninformative priors are flat, or proportional to a uniform on some subset of the real line and therefore improper. It may be superficially tempting to set a uniform prior on a large range to ensure posterior property, but the major problem is that a flat prior may be informative in a different parametrization, as the following example suggests.</p>
<div id="exm-scaleflatprior" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.5 </strong></span>Consider the parameter <span class="math inline">\(\log(\tau) \in \mathbb{R}\)</span> and the prior <span class="math inline">\(p( \log \tau) \propto 1\)</span>. If we reparametrize the model in terms of <span class="math inline">\(\tau\)</span>, the new prior (including the Jacobian of the transformation) is <span class="math inline">\(\tau^{-1}\)</span></p>
</div>
<p>Some priors are standard and widely used. In location scale families with location <span class="math inline">\(\nu\)</span> and scale <span class="math inline">\(\tau\)</span>, the density is such that <span class="math display">\[\begin{align*}
f(x; \nu, \tau) =  \frac{1}{\tau} f\left(\frac{x - \nu}{\tau}\right), \qquad \nu \in \mathbb{R}, \tau &gt;0.
\end{align*}\]</span> We thus wish to have a prior so that <span class="math inline">\(p(\tau) = c^{-1}p(\tau/c)\)</span> for any scaling <span class="math inline">\(c&gt;0\)</span>, whence it follows that <span class="math inline">\(p(\tau) \propto \tau^{-1}\)</span>, which is uniform on the log scale.</p>
<p>The priors <span class="math inline">\(p(\nu) \propto 1\)</span> and <span class="math inline">\(p(\tau) \propto \tau^{-1}\)</span> are both improper but lead to location and scale invariance, hence that the result is the same regardless of the units of measurement.</p>
<div id="def-jeffreys" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (Jeffrey’s prior) </strong></span>In single parameter models, taking a prior function for <span class="math inline">\(\theta\)</span> proportional to the square root of the determinant of the information matrix <span class="math inline">\(p(\theta) \propto \imath(\theta)\)</span> yields a prior that is invariant to parametrization, so that inferences conducted in different parametrizations are equivalent.</p>
<!--
From notes of M. Jordan for MATH 260.
-->
<p>To see this, consider a bijective transformation <span class="math inline">\(\theta \mapsto \vartheta\)</span>. Under the reparametrized model and suitable regularity conditions<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, the chain rule implies that <span class="math display">\[\begin{align*}
i(\vartheta) &amp;= - \mathsf{E} \left(\frac{\partial^2 \ell(\vartheta)}{\partial^2 \vartheta}\right)
\\&amp;= - \mathsf{E}\left(\frac{\partial^2 \ell(\theta)}{\partial \theta^2}\right) \left( \frac{\mathrm{d} \theta}{\mathrm{d} \vartheta} \right)^2 + \mathsf{E}\left(\frac{\partial \ell(\theta)}{\partial \theta}\right) \frac{\mathrm{d}^2 \theta}{\mathrm{d} \vartheta^2}
\end{align*}\]</span> Since the score has mean zero, <span class="math inline">\(\mathsf{E}\left\{\partial \ell(\theta)/\partial \theta\right\}=0\)</span>, the rightmost term vanishes. We can thus relate the Fisher information in both parametrizations, with <span class="math display">\[\begin{align*}
\imath^{1/2}(\vartheta) = \imath^{1/2}(\theta) \left| \frac{\mathrm{d} \theta}{\mathrm{d} \vartheta} \right|,
\end{align*}\]</span> implying invariance.</p>
<p>Most of the times, Jeffrey’s prior is improper. For the binomial model, it can be viewed as a limiting conjugate beta prior with <span class="math inline">\(\alpha, \beta\to 0\)</span>). Unfortunately, in multiparameter models, the system isn’t invariant to reparametrization if we consider the determinant of the Fisher information.</p>
</div>
<div id="exm-jeffreysbinom" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.6 </strong></span>Consider the binomial distribution <span class="math inline">\(f(y; \theta, n) \propto \theta^y(1-\theta)^{n-y}\mathsf{I}_{\theta \in [0,1]}\)</span>. The negative of the second derivative of the log likelihood with respect to <span class="math inline">\(p\)</span> is <span class="math inline">\(\jmath(\theta) = - \partial^2 \ell(\theta; y) / \partial \theta^2 = y/\theta^2 + (1-y)/(1-\theta)^2\)</span> and since <span class="math inline">\(\mathsf{E}(Y)=n\theta\)</span>, thus the Fisher information is <span class="math inline">\(\imath = \mathsf{E}\{\jmath(\theta)\}=n/\theta + n/(1-\theta) = n/\{\theta(1-\theta)\}\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Jeffrey’s prior is thus <span class="math inline">\(p(\theta) \propto \theta^{-1}(1-\theta)^{-1}\)</span>.</p>
</div>
<div id="exer-jeffreysnormal">
<p>Check that for the Gaussian distribution <span class="math inline">\(\mathsf{No}(\mu, \sigma^2)\)</span>, the Jeffrey’s prior obtained by treating each parameter in turn, fixing the value of the other, are <span class="math inline">\(p(\mu) \propto 1\)</span> and <span class="math inline">\(p(\sigma) \propto 1/\sigma\)</span>, which also correspond to the default uninformative priors for location-scale families.</p>
</div>
<div id="exm-jeffreyspoisson" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.7 </strong></span>The Poisson distribution with <span class="math inline">\(\ell(\lambda) \propto -\lambda + y\log \lambda\)</span>, with second derivative <span class="math inline">\(-\partial^2 \ell(\lambda)/\partial \lambda^2 = y/\lambda^2\)</span>. Since the mean of the Poisson distribution is <span class="math inline">\(\lambda\)</span>, the Fisher information is <span class="math inline">\(\imath(\lambda) = \lambda^{-1}\)</span> and Jeffrey’s prior is <span class="math inline">\(\lambda^{-1/2}\)</span>.</p>
</div>
</section>
<section id="expert-knowledge" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="expert-knowledge"><span class="header-section-number">2.3</span> Expert knowledge</h2>
<p>Th prior distribution may have parameters themselves that need to be specified by experts. One may also wish to add another layer and set an hyperprior distribution on the parameters, resulting in a hierarchical model.</p>
<p>Setting parameters of priors is often done by reparametrizing the latter in terms of moments. Sometimes, it may be easier to set priors in a different scale where subject-matter expertise is most easily elicited.</p>
<div id="exm-colestawn" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.8 </strong></span>The generalized extreme value distribution arises as the limiting distribution for the maximum of <span class="math inline">\(m\)</span> independent observations. The <span class="math inline">\(\mathsf{GEV}(\mu, \sigma, \xi)\)</span> distribution is a location-scale with distribution function <span class="math display">\[\begin{align*}
F(x) = \exp\left\{ - \left(1+\xi(x-\mu)/\sigma\right)^{-1/\xi}_{+}\right\}
\end{align*}\]</span> where <span class="math inline">\(x_{+} = \max\{0, x\}\)</span>.</p>
<p>Inverting the distribution function yields the quantile function <span class="math display">\[\begin{align*}
Q(p) \mu + \sigma \frac{(-\log p)^{-\xi}-1}{\xi}
\end{align*}\]</span></p>
<p>In environmental data, we often model annual maximum. Engineering designs are often specified in terms of the <span class="math inline">\(k\)</span>-year return levels, defined as the quantile of the annual maximum exceeded with probability <span class="math inline">\(1/k\)</span> in any given year. Using a <span class="math inline">\(\mathsf{GEV}\)</span> for annual maximum, <span class="citation" data-cites="Coles.Tawn:1996">Coles and Tawn (<a href="references.html#ref-Coles.Tawn:1996" role="doc-biblioref">1996</a>)</span> proposed modelling annual daily rainfall and specifying a prior on the quantile scale <span class="math inline">\(q_1 &lt; q_2 &lt; q_3\)</span> for tail probabilities <span class="math inline">\(p_1&gt; p_2 &gt; p_3\)</span>. To deal with the ordering constraints, gamma priors are imposed on the differences <span class="math inline">\(q_1 - o \sim \mathsf{Ga}(\alpha_1, \beta_1)\)</span>, <span class="math inline">\(q_2 - q_1 \sim \mathsf{Ga}(\alpha_2, \beta_2)\)</span> and <span class="math inline">\(q_3-q_2 \sim \mathsf{Ga}(\alpha_3, \beta_3)\)</span>, where <span class="math inline">\(o\)</span> is the lower bound of the support. The prior is thus of the form</p>
<p><span class="math display">\[\begin{align*}
p(\boldsymbol{q}) \propto q_1^{\alpha_1-1}\exp(-\beta_1 q_1) \prod_{i=2}^3 (q_i-q_{i-1}^{\alpha_i-1} \exp\{\beta_i(q_i-q_{i-1})\}.
\end{align*}\]</span> where <span class="math inline">\(0 \leq q_1 \leq q_2 \leq q_3\)</span>. We can then relate the prior parameters to moments.</p>
<p>Consider the annual maximum rainfall in Abisko, Sweden.</p>
</div>
<div id="exm-priorsim" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.9 (Prior simulation) </strong></span>Are the prior reasonable? One way to see this is to sample values from the priors and generate new observations.</p>
</div>
<div id="exm-priosim-gev" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.10 </strong></span>We can specify gamma back-transform them to location <span class="math inline">\(\mu\)</span>, scale <span class="math inline">\(\sigma\)</span> and shape <span class="math inline">\(\xi\)</span> and simulate observations from the <span class="math inline">\(\mathsf{GEV}(\mu, \sigma, \xi)\)</span> and compare them to observations.</p>
</div>

<!--

Hierarchical linear model with half-t prior

Prior elicitation may require [expert knowledge](https://arxiv.org/abs/2112.01380).


Quantile priors of [Coles and Tawn](http://www.jstor.org/stable/2986068) (using `revdbayes`)


Are my priors reasonable? Use prior predictive distribution to assess the plausibility
comparing prior to posterior standard deviations, e.g., Nott et al. (2020)

Example: simple linear regression slope (height/weight) of Figure 4.5 in McElreath


Improper priors may lead to improper posterior: stick with proper distributions unless you know what you are doing

Penalized complexity prior

Maximum domain information

Sensivity analysis and asymptotic effect

Consensus of opinion: expert opinion and mixture

-->


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Alexander:2023" class="csl-entry" role="listitem">
Alexander, Rohan. 2023. <em>Telling Stories with Data: With Applications in <span>R</span></em>. Boca Raton, FL: CRC Press.
</div>
<div id="ref-Coles.Tawn:1996" class="csl-entry" role="listitem">
Coles, Stuart G., and Jonathan A. Tawn. 1996. <span>“A <span>B</span>ayesian Analysis of Extreme Rainfall Data.”</span> <em>Journal of the Royal Statistical Society. Series C (Applied Statistics)</em> 45 (4): 463–78. <a href="https://doi.org/10.2307/2986068">https://doi.org/10.2307/2986068</a>.
</div>
<div id="ref-Matias:2021" class="csl-entry" role="listitem">
Matias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2021. <span>“The <span>U</span>pworthy <span>R</span>esearch <span>A</span>rchive, a Time Series of 32,487 Experiments in <span>U.S.</span> Media.”</span> <em>Scientific Data</em> 8 (195). <a href="https://doi.org/10.1038/s41597-021-00934-7">https://doi.org/10.1038/s41597-021-00934-7</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>This simply means that the precision <span class="math inline">\(\sigma^{-2}\)</span>, the reciprocal of the variance, has a gamma distribution with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Using Bartlett’s identity; Fisher consistency can be established using the dominated convergence theorem.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>The Fisher information is linear in the sample size for independent and identically distributed data.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./introduction.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./references.html" class="pagination-link">
        <span class="nav-page-text">References</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>