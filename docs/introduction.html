<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Bayesian modelling - 1&nbsp; Bayesics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./priors.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./introduction.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayesics</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Bayesian modelling</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/lbelzile/math80601a/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./MATH80601A-bayesmod.pdf" rel="" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayesics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./priors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#probability-and-frequency" id="toc-probability-and-frequency" class="nav-link active" data-scroll-target="#probability-and-frequency"><span class="header-section-number">1.1</span> Probability and frequency</a></li>
  <li><a href="#posterior-distribution" id="toc-posterior-distribution" class="nav-link" data-scroll-target="#posterior-distribution"><span class="header-section-number">1.2</span> Posterior distribution</a></li>
  <li><a href="#posterior-predictive-distribution" id="toc-posterior-predictive-distribution" class="nav-link" data-scroll-target="#posterior-predictive-distribution"><span class="header-section-number">1.3</span> Posterior predictive distribution</a></li>
  <li><a href="#summarizing-posterior-distributions" id="toc-summarizing-posterior-distributions" class="nav-link" data-scroll-target="#summarizing-posterior-distributions"><span class="header-section-number">1.4</span> Summarizing posterior distributions</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/lbelzile/math80601a/edit/main/introduction.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Bayesics</span></h1>
</header>

<p>The Bayesian paradigm is an inferential framework that is used widespread in data science. Numerical challenges that prevented it’s widespread adoption until the 90’s, when the Markov chain Monte Carlo revolution allowed models estimation.</p>
<p>Bayesian inference, which builds on likelihood-based inference, offers a natural framework for prediction and for uncertainty quantification. The interpretation is more natural than that of classical frequentists methods, and it is more easy to generalized models to complex settings, using notably hierarchical constructions. The main source of controversy is the role of the prior distribution, which allows one to incorporate subject-matter expertise but leads to different inferences being drawn by different practitioners; this subjectivity is not to the taste of many and has been the subject of many controversies.</p>
<p>The Bayesian paradigm includes multiples notions that are not covered in undergraduate introductory courses. The purpose of this chapter is to introduce these concepts and put them in perspective; the reader is assumed to be familiar with basics of likelihood-based inference. We begin with a discussion of the notion of probability, then define priors, posterior distributions, marginal likelihood and posterior predictive distributions. We focus on the interpretation of posterior distributions and explain how to summarize the posterior, leading leading to definitions of high posterior density region, credible intervals, posterior mode for cases where we either have a (correlated) sample from the posterior, or else have access to the whole distribution. Several notions, including sequentiality, prior elicitation and estimation of the marginal likelihood, are mentioned in passing. A brief discussion of Bayesian hypothesis testing (and alternatives) is presented.</p>
<section id="probability-and-frequency" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="probability-and-frequency"><span class="header-section-number">1.1</span> Probability and frequency</h2>
<p>In classical (frequentist) parametric statistic, we treat observations <span class="math inline">\(\boldsymbol{Y}\)</span> as realizations of a distribution whose parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> are unknown. All of the information about parameters is encoded by the likelihood function.</p>
<p>The interpretation of probability in the classical statistic is in terms of long run frequency, which is why we term this approach frequentist statistic. Think of a fair die: when we state that values <span class="math inline">\(\{1, \ldots, 6\}\)</span> are equiprobable, we mean that repeatedly tossing the die should result, in large sample, in each outcome being realized roughly <span class="math inline">\(1/6\)</span> of the time (the symmetry of the object also implies that each facet should be equally likely to lie face up). This interpretation also carries over to confidence intervals: a <span class="math inline">\((1-\alpha)\)</span> confidence interval either contains the true parameter value or it doesn’t, so the probability level <span class="math inline">\((1-\alpha)\)</span> is only the long-run proportion of intervals created by the procedure that should contain the true fixed value, not the probability that a single interval contains the true value. This is counter-intuitive to most.</p>
<p>In practice, the true value of the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> vector is unknown to the practitioner, thus uncertain: Bayesians would argue that we should treat the latter as a random quantity rather than a fixed constant. Since different people may have different knowledge about these potential values, the prior knowledge is a form of <strong>subjective probability</strong>. For example, if you play cards, one person may have recorded the previous cards that were played, whereas other may not. They thus assign different probability of certain cards being played. In Bayesian inference, we consider <span class="math inline">\(\boldsymbol{\theta}\)</span> as random variables to reflect our lack of knowledge about potential values taken. Italian scientist Bruno de Finetti, who is famous for the claim ``Probability does not exist’’, stated in the preface of <span class="citation" data-cites="deFinetti:1974">Finetti (<a href="references.html#ref-deFinetti:1974" role="doc-biblioref">1974</a>)</span>:</p>
<blockquote class="blockquote">
<p>Probabilistic reasoning — always to be understood as subjective — merely stems from our being uncertain about something. It makes no difference whether the uncertainty relates to an unforseeable future, or to an unnoticed past, or to a past doubtfully reported or forgotten: it may even relate to something more or less knowable (by means of a computation, a logical deduction, etc.) but for which we are not willing or able tho make the effort; and so on […] The only relevant thing is uncertainty — the extent of our knowledge and ignorance. The actual fact of whether or not the events considered are in some sense <em>determined</em>, or known by other people, and so on, is of no consequence.</p>
</blockquote>
<p>On page 3, de Finetti continues <span class="citation" data-cites="deFinetti:1974">(<a href="references.html#ref-deFinetti:1974" role="doc-biblioref">Finetti 1974</a>)</span></p>
<blockquote class="blockquote">
<p>only subjective probabilities exist — i.e., the degree of belief in the occurrence of an event attributed by a given person at a given instant and with a given set of information.</p>
</blockquote>
</section>
<section id="posterior-distribution" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="posterior-distribution"><span class="header-section-number">1.2</span> Posterior distribution</h2>
<p>We consider a parametric model with parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> defined on <span class="math inline">\(\boldsymbol{\Theta} \subseteq \mathbb{R}^p\)</span>. In Bayesian learning, we adjoin to the likelihood <span class="math inline">\(\mathcal{L}(\boldsymbol{\theta}; \boldsymbol{y}) \equiv p(\boldsymbol{y} \mid \boldsymbol{\theta})\)</span> a <strong>prior</strong> function <span class="math inline">\(p(\boldsymbol{\theta})\)</span> that reflects the prior knowledge about potential values taken by the <span class="math inline">\(p\)</span>-dimensional parameter vector, before observing the data <span class="math inline">\(\boldsymbol{y}\)</span>. The prior makes <span class="math inline">\(\boldsymbol{\theta}\)</span> random and the distribution of the parameter reflects our uncertainty about the true value of the model parameters.</p>
<p>In a Bayesian analysis, observations are random variables but inference is performed conditional on the observed sample values. By Bayes’ theorem, our target is therefore the posterior density <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span>, defined as</p>
<p><span id="eq-posterior"><span class="math display">\[
\underbracket[0.25pt]{p(\boldsymbol{\theta} \mid \boldsymbol{y})}_{\text{posterior}} = \frac{\overbracket[0.25pt]{p(\boldsymbol{y} \mid \boldsymbol{\theta})}^{\text{likelihood}} \times  \overbracket[0.25pt]{p(\boldsymbol{\theta})}^{\text{prior}}}{\underbracket[0.25pt]{\int p(\boldsymbol{y} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}}_{\text{marginal likelihood }p(\boldsymbol{y})}}.
\tag{1.1}\]</span></span></p>
<p>The posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y})\)</span> is proportional, as a function of <span class="math inline">\(\theta,\)</span> to the product of the likelihood and the prior function.</p>
<p>For the posterior to be <strong>proper</strong>, we need the product of the prior and the likelihood on the right hand side to be integrable as a function of <span class="math inline">\(\boldsymbol{\theta}\)</span> over the parameter domain <span class="math inline">\(\boldsymbol{\Theta}\)</span>. The integral in the denominator, termed marginal likelihood and denoted <span class="math inline">\(p(\boldsymbol{y}) = \mathsf{E}_{\boldsymbol{\theta}}\{p(\boldsymbol{y} \mid \boldsymbol{\theta})\}\)</span>. The denominator of <a href="#eq-posterior">Equation&nbsp;<span>1.1</span></a> is a normalizing constant, making the posterior a valid density.</p>
<p>If <span class="math inline">\(\boldsymbol{\theta}\)</span> is low dimensional, numerical integration such as quadrature methods can be used to compute the marginal likelihood.</p>
<p>To fix ideas, we consider next a simple one-parameter model where the marginal likelihood can be computed explicitly.</p>
<div id="exm-betabinomconjugate" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Binomial model with beta prior) </strong></span>Consider a binomial likelihood with probability of success <span class="math inline">\(p\)</span> and <span class="math inline">\(n\)</span> trials, <span class="math inline">\(Y \sim \mathsf{Bin}(n, p)\)</span>. If we take a beta prior, <span class="math inline">\(p \sim \mathsf{Be}(\alpha, \beta)\)</span> and observe <span class="math inline">\(k\)</span> successes, the posterior is <span class="math display">\[\begin{align*}
p(\theta \mid y = k) &amp;\propto \binom{n}{k} p^k (1-p)^{n-k} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}p^{\alpha-1} (1-p)^{\beta-1}
\\&amp;\stackrel{p}{\propto} p^{k+\alpha-1}(1-p)^{n-k+\beta-1}
\end{align*}\]</span> and is <span class="math display">\[\int_{0}^{1} p^{k+\alpha-1}(1-p)^{n-k+\beta-1}\mathrm{d} p = \frac{\Gamma(k+\alpha)\Gamma(n-k+\beta)}{\Gamma(n+\alpha+\beta)},\]</span> a Beta function. Since we need only to keep track of the terms that are function of the parameter <span class="math inline">\(p\)</span>, we could recognize directly that the posterior distribution is <span class="math inline">\(\mathsf{Be}(k+\alpha, n-k+\beta)\)</span> and deduce the normalizing constant from there.</p>
<p>If <span class="math inline">\(Y \sim \mathsf{Bin}(n, p)\)</span>, the expected number of success is <span class="math inline">\(np\)</span> and the expected number of failures <span class="math inline">\(n(1-p)\)</span> and so the likelihood contribution, relative to the prior, will dominate as the sample size <span class="math inline">\(n\)</span> grows.</p>
<p>Another way to see this is to track moments (expectation, variance, etc.) The Beta distribution, whose density is <span class="math inline">\(f(x; \alpha, \beta) \propto x^{\alpha-1} (1-x)^{\beta-1}\)</span>, has expectation <span class="math inline">\(\alpha/(\alpha+\beta)\)</span> and variance <span class="math inline">\(\alpha\beta/\{(\alpha+\beta)^2(\alpha+\beta+1)\}\)</span>. The posterior mean is <span class="math display">\[\begin{align*}
\mathsf{E}(p \mid y) = w\frac{y}{n} + (1-w) \frac{a}{a+b},
\qquad w = \frac{n}{n+a+b},
\end{align*}\]</span> a weighted average of the maximum likelihood estimator and the prior mean. We can think of the parameter <span class="math inline">\(\alpha\)</span> (respectively <span class="math inline">\(\beta\)</span>) as representing the prior number of success (resp. failures).</p>
<p><a href="#fig-betabinom">Figure&nbsp;<span>1.1</span></a> shows three different posterior distributions with different beta priors: the first prior, which favors values closer to 1/2, leads to a more peaked posterior density, contrary to the second which is symmetric, but concentrated toward more extreme values near endpoints of the support. The rightmost panel is truncated: as such, the posterior is zero for any value of <span class="math inline">\(p\)</span> beyond 1/2 and so the posterior mode may be close to the endpoint of the prior. The influence of such a prior will not necessarily vanish as sample size and should be avoided, unless there are compelling reasons for restricting the domain.</p>
<div class="cell" data-hash="introduction_cache/html/fig-betabinom_505108ea87373d8e9cd291e298ca34de">
<div class="cell-output-display">
<div id="fig-betabinom" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="introduction_files/figure-html/fig-betabinom-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;1.1: Scaled binomial likelihood for six successes out of 14 trials, with <span class="math inline">\(\mathsf{Beta}(3/2, 3/2)\)</span> prior (left), <span class="math inline">\(\mathsf{Beta}(1/4, 1/4)\)</span> (middle) and truncated uniform on <span class="math inline">\([0,1/2]\)</span> (right), with the corresponding posterior distributions.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em> (Proportionality). </span>Any term appearing in the likelihood times prior function that does not depend on parameters can be omitted since they will be absorbed by the normalizing constant. This makes it useful to compute normalizing constants or likelihood ratios.</p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>An alternative parametrization for the beta distribution sets <span class="math inline">\(\alpha=\mu \kappa\)</span>, <span class="math inline">\(\beta = (1-\mu)\kappa\)</span> for <span class="math inline">\(\mu \in (0,1)\)</span> and <span class="math inline">\(\kappa&gt;0\)</span>, so that the model is parametrized directly in terms of mean <span class="math inline">\(\mu\)</span>, with <span class="math inline">\(\kappa\)</span> capturing the dispersion.</p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>A density integrates to 1 over the range of possible outcomes, but there is no guarantee that the likelihood function, as a function of <span class="math inline">\(\boldsymbol{\theta}\)</span>, integrates to one over the parameter domain <span class="math inline">\(\boldsymbol{\Theta}\)</span>.</p>
<p>For example, the binomial likelihood with <span class="math inline">\(n\)</span> trials and <span class="math inline">\(k\)</span> successes satisfies <span class="math display">\[\int_0^1 \binom{n}{k}p^k(1-p)^{n-k} \mathrm{d} p = \frac{1}{n+1}.\]</span></p>
<p>Moreover, the binomial distribution is discrete (supported on <span class="math inline">\(0, \ldots, n\)</span>), whereas the likelihood is continuous as a function of the probability of success, as evidenced by <a href="#fig-binom-massvslik">Figure&nbsp;<span>1.2</span></a></p>
<div class="cell" data-hash="introduction_cache/html/fig-binom-massvslik_8cd398aa0e9f9c24b90b44250525a3e0">
<div class="cell-output-display">
<div id="fig-binom-massvslik" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="introduction_files/figure-html/fig-binom-massvslik-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;1.2: Binomial mass function (left) and scaled likelihood function (right).</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="prp-sequentiality" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1.1 (Sequentiality and Bayesian updating) </strong></span>The likelihood is invariant to the order of the observations if they are independent Thus, if we consider two blocks of observations <span class="math inline">\(\boldsymbol{y}_1\)</span> and <span class="math inline">\(\boldsymbol{y}_2\)</span> <span class="math display">\[p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \boldsymbol{y}_2) = p(\boldsymbol{\theta} \mid \boldsymbol{y}_1) p(\boldsymbol{\theta} \mid \boldsymbol{y}_2),\]</span> so it makes no difference if we treat data all at once or in blocks. More generally, for data exhibiting spatial or serial dependence, it makes sense to consider rather the conditional (sequential) decomposition <span class="math display">\[f(\boldsymbol{y}; \boldsymbol{\theta}) = f(\boldsymbol{y}_1; \boldsymbol{\theta}) f(\boldsymbol{y}_2; \boldsymbol{\theta}, \boldsymbol{y}_1) \cdots f(\boldsymbol{y}_n; \boldsymbol{\theta}, \boldsymbol{y}_1, \ldots, \boldsymbol{y}_{n-1})\]</span> where <span class="math inline">\(f(\boldsymbol{y}_k; \boldsymbol{y}_1, \ldots, \boldsymbol{y}_{k-1})\)</span> denotes the conditional density function given observations <span class="math inline">\(\boldsymbol{y}_1, \ldots, \boldsymbol{y}_{k-1}\)</span>.</p>
<p>By Bayes’ rule, we can consider <em>updating</em> the posterior by adding terms to the likelihood, noting that <span class="math display">\[\begin{align*}
p(\boldsymbol{\theta} \mid \boldsymbol{y}_1, \boldsymbol{y}_2) \propto p(\boldsymbol{y}_2 \mid \boldsymbol{y}_1, \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid \boldsymbol{y}_1)
\end{align*}\]</span> which amounts to treating the posterior <span class="math inline">\(p(\boldsymbol{\theta} \mid \boldsymbol{y}_1)\)</span> as a prior. If data are exchangeable, the order in which observations are collected and the order of the belief updating is irrelevant to the full posterior. <a href="#fig-sequential">Figure&nbsp;<span>1.3</span></a> shows how the posterior becomes gradually closer to the scaled likelihood as we increase the sample size, and the posterior mode moves towards the true value of the parameter (here 0.3).</p>
<div class="cell" data-hash="introduction_cache/html/fig-sequential_26f75c2fe4649995d1eef70232b21b27">
<div class="cell-output-display">
<div id="fig-sequential" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="introduction_files/figure-html/fig-sequential-1.png" class="img-fluid figure-img" width="1152"></p>
<figcaption class="figure-caption">Figure&nbsp;1.3: Beta posterior and binomial likelihood with a uniform prior for increasing number of observations (from left to right) out of a total of 100 trials.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div id="exm-numericalintegration" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 </strong></span>While we can calculate analytically the value of the normalizing constant for the beta-binomial model, we could also for arbitrary priors use numerical integration or Monte Carlo methods in the event the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> is low-dimensional.</p>
<p>While estimation of the normalizing constant is possible in simple models, the following highlights some challenges that are worth keeping in mind. In a model for discrete data (that is, assigning probability mass to a countable set of outcomes), the terms in the likelihood are probabilities and thus the likelihood becomes smaller as we gather more observations (since we multiply terms between zero or one). The marginal likelihood term becomes smaller and smaller, so it’s reciprocal is big and this can lead to arithmetic underflow.</p>
<div class="cell" data-hash="introduction_cache/html/unnamed-chunk-4_87c51332d23d66b867c881438d928d40">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> 6L <span class="co"># number of successes </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> 14L <span class="co"># number of trials</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> beta <span class="ot">&lt;-</span> <span class="fl">1.5</span> <span class="co"># prior parameters</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>unnormalized_posterior <span class="ot">&lt;-</span> <span class="cf">function</span>(p){</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  p<span class="sc">^</span>(k<span class="sc">+</span>alpha<span class="dv">-1</span>) <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">^</span>(n<span class="sc">-</span>k <span class="sc">+</span> beta <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="at">f =</span> unnormalized_posterior,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>          <span class="at">lower =</span> <span class="dv">0</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>          <span class="at">upper =</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.066906e-05 with absolute error &lt; 1e-12</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare with known constant</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">beta</span>(k <span class="sc">+</span> alpha, n <span class="sc">-</span> k <span class="sc">+</span> beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.066906e-05</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Monte Carlo integration</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">unnormalized_posterior</span>(<span class="fu">runif</span>(<span class="fl">1e5</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.064067e-05</code></pre>
</div>
</div>
</div>
<p>When <span class="math inline">\(\boldsymbol{\theta}\)</span> is high-dimensional, the marginal likelihood is intractable. This is one of the main challenges of Bayesian statistics and the popularity and applicability has grown drastically with the development and popularity of numerical algorithms, following th publication of <span class="citation" data-cites="Geman.Geman:1984">Geman and Geman (<a href="references.html#ref-Geman.Geman:1984" role="doc-biblioref">1984</a>)</span> and <span class="citation" data-cites="Gelfand.Smith:1990">Gelfand and Smith (<a href="references.html#ref-Gelfand.Smith:1990" role="doc-biblioref">1990</a>)</span>. Markov chain Monte Carlo methods circumvent the calculation of the denominator by drawing approximate samples from the posterior.</p>
</section>
<section id="posterior-predictive-distribution" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="posterior-predictive-distribution"><span class="header-section-number">1.3</span> Posterior predictive distribution</h2>
</section>
<section id="summarizing-posterior-distributions" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="summarizing-posterior-distributions"><span class="header-section-number">1.4</span> Summarizing posterior distributions</h2>
<p>Most of the field revolves around the creation of algorithms that either circumvent the calculation of the normalizing constant (notably Monte Carlo and Markov chain Monte Carlo methods) or else provide accurate numerical approximation of the posterior pointwise, including for marginalizing out all but one parameters (integrated nested Laplace approximations, variational inference, etc.)</p>
<p>The target of inference is the whole posterior distribution, from which we can extract any summary of interest. For example, if we consider the beta-binomial model, we can assess the posterior probability <span class="math inline">\(\Pr(P &gt; c)\)</span> simply by calculating the area under the posterior density until <span class="math inline">\(c\)</span>. In certain settings, however it will be useful to provide moments or other characteristics of the distribution, notably since visualization of a (potentially high-dimensional) object is not easy.</p>
<p>The output of the Bayesian learning problem will be either of:</p>
<ol type="1">
<li>a fully characterized distribution</li>
<li>a numerical approximation to the posterior distribution (pointwise)</li>
<li>an exact or approximate sample drawn from the posterior distribution</li>
</ol>
<p>Point estimators are usually in terms of central tendency. We can return the expected value or mean of the posterior, quantiles such as the median or the mode, the value at which the posterior density is highest. These functionals may correspond to potentially different values, as shown in the left-panel of <a href="#fig-central-moments">Figure&nbsp;<span>1.4</span></a>. For multimodal distributions, the mode is likely the better choice.</p>
<p>If we know the distribution, we can optimize to find the mode or else return the value for the pointwise evaluation on a grid at which the density achieves it’s maximum.</p>
<div class="cell" data-hash="introduction_cache/html/fig-central-moments_09d7385e76b50a09cf3152f189d34c6f">
<div class="cell-output-display">
<div id="fig-central-moments" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="introduction_files/figure-html/fig-central-moments-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">Figure&nbsp;1.4: Point estimators from a left-skewed distribution (left) and from a multimodal distribution (right).</figcaption>
</figure>
</div>
</div>
</div>
<p>Often, we will also be interested in the marginal posterior distribution of each component <span class="math inline">\(\theta_j\)</span> in turn (<span class="math inline">\(j=1, \ldots, p\)</span>). To get these, we carry out additional integration steps, <span class="math display">\[p(\theta_j \mid \boldsymbol{y}) = \int p(\boldsymbol{\theta} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\theta}_{-j}.\]</span></p>
<!--

TODO:

- loss function and EVA example with generalized Pareto (FIG)
- HPD and percentile intervals (FIG)
- Summaries using Monte Carlo (quantiles)

-->


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-deFinetti:1974" class="csl-entry" role="listitem">
Finetti, Bruno de. 1974. <em>Theory of Probability: A Critical Introductory Treatment</em>. Vol. 1. New York: Wiley.
</div>
<div id="ref-Gelfand.Smith:1990" class="csl-entry" role="listitem">
Gelfand, Alan E., and Adrian F. M. Smith. 1990. <span>“Sampling-Based Approaches to Calculating Marginal Densities.”</span> <em>Journal of the American Statistical Association</em> 85 (410): 398–409. <a href="https://doi.org/10.1080/01621459.1990.10476213">https://doi.org/10.1080/01621459.1990.10476213</a>.
</div>
<div id="ref-Geman.Geman:1984" class="csl-entry" role="listitem">
Geman, Stuart, and Donald Geman. 1984. <span>“Stochastic Relaxation, <span>G</span>ibbs Distributions, and the <span>B</span>ayesian Restoration of Images.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> PAMI-6 (6): 721–41. <a href="https://doi.org/10.1109/TPAMI.1984.4767596">https://doi.org/10.1109/TPAMI.1984.4767596</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Welcome</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./priors.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Priors</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>