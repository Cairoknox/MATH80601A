[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian modelling",
    "section": "",
    "text": "Welcome\nThis book is a web complement to MATH 80601A Bayesian modelling, a graduate course offered at HEC Montréal.\nThese notes are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License and were last compiled on Thursday, August 24 2023.\nThe objective of the course is to provide a hands on introduction to Bayesian data analysis. The course will cover the formulation, evaluation and comparison of Bayesian models through examples and real-data applications."
  },
  {
    "objectID": "introduction.html#bayes-theorem",
    "href": "introduction.html#bayes-theorem",
    "title": "1  Introduction",
    "section": "1.1 Bayes theorem",
    "text": "1.1 Bayes theorem\nDenote by \\(p(X) \\equiv\\Pr(X)\\) denotes the marginal density of \\(X\\), \\(p(X \\mid Y)\\) the conditional of \\(X\\) given \\(Y\\) and \\(p(X, Y)\\) the joint density. Bayes’ theorem states that \\[\\begin{align*}\np(X = x \\mid Y=y) = \\frac{p(Y = y \\mid X = x)p(X=x)}{p(Y=y)}\n\\end{align*}\\]\nIn the case of discrete random variable \\(X\\) with support \\(\\mathcal{X},\\) the denominator can be evaluated using the law of total probability as \\[\\Pr(Y=y) = \\sum_{x \\in \\mathcal{X}}\\Pr(Y =y \\mid X=x)\\Pr(X=x).\\]\n\nExample 1.1 Back in January 2021, the Quebec government was debating whether or not to distribute antigen rapid test, with strong reluctance from authorities given the paucity of available resources and the poor sensitivity.\nA Swiss study analyse the efficiency of rapid antigen tests, comparing them to repeated polymerase chain reaction (PCR) test output, taken as benchmark (Jegerlehner et al. 2021). The results are presented in Table 1.1\n\n\nTable 1.1: Confusion matrix of Covid test results for PCR tests versus rapid antigen tests, from Jegerlehner et al. (2021).\n\n\n\nPCR \\(+\\)\nPCR \\(-\\)\n\n\n\n\nrapid \\(+\\)\n92\n2\n\n\nrapid \\(-\\)\n49\n1319\n\n\ntotal\n141\n1321\n\n\n\n\nEstimated seropositivity at the end of January 2021 according to projections of the Institute for Health Metrics and Evaluation (IHME) of 8.18M out of 38M inhabitants (Mathieu et al. 2020), a prevalence of 21.4%. Assuming the latter holds uniformly over the country, what is the probability of having Covid if I get a negative result to a rapid test?\nLet \\(R^{-}\\) (\\(R^{+}\\)) denote a negative (positive) rapid test result and \\(C^{+}\\) (\\(C^{-}\\)) Covid positivity (negativity). Bayes’ formula gives \\[\\begin{align*}\n\\Pr(C^{+} \\mid R^{-}) & = \\frac{\\Pr(R^{-} \\mid C^{+})\\Pr(C^{+})}{\\Pr(R^{-} \\mid C^{+})\\Pr(C^{+}) + \\Pr(R^{-} \\mid C^{-})\\Pr(C^{-})} \\\\&=\n\\frac{49/141 \\cdot 0.214}{49/141 \\cdot 0.214 + 1319/1321 \\cdot 0.786}\n\\end{align*}\\] so there is a small, but non-negligible probability of 8.66% that the rapid test result is misleading. Jegerlehner et al. (2021) indeed found that the sensitivity was 65.3% among symptomatic individuals, but dropped down to 44% for asymptomatic cases. This may have fueled government experts skepticism."
  },
  {
    "objectID": "introduction.html#probability-and-frequency",
    "href": "introduction.html#probability-and-frequency",
    "title": "1  Introduction",
    "section": "1.2 Probability and frequency",
    "text": "1.2 Probability and frequency\nIn classical (frequentist) parametric statistic, we treat observations \\(\\boldsymbol{Y}\\) as realizations of a distribution whose parameters \\(\\boldsymbol{\\theta}\\) are unknown. All of the information about parameters is encoded by the likelihood function, which is optimized numerically or analytically to find the maximum likelihood estimator. Large-sample theory shows that the resulting estimator is asymptotically normal under regularity conditions.\nThe interpretation of probability in the classical statistic is understood in terms of long run frequency, which is why we call this approach frequentist statistic. Think of a fair die: when we state that values \\(\\{1, \\ldots, 6\\}\\) are equiprobable, we mean that repeatedly tossing the die should result, in large sample, in each outcome being realized roughly \\(1/6\\) of the time (the symmetry of the object also implies that each facet should be equally likely to lie face up). This interpretation also carries over to confidence intervals: a \\((1-\\alpha)\\) confidence interval either contains the true parameter value or it doesn’t, so the probability level \\((1-\\alpha)\\) is only the long-run proportion of intervals created by the procedure that should contain the true fixed value, not the probability that a single interval contains the true value. This is counter-intuitive to most.\nIn practice, the true value of the parameter \\(\\boldsymbol{\\theta}\\) vector is unknown to the practitioner, thus uncertain: Bayesians would argue that we should treat the latter as a random quantity rather than a fixed constant. Since different people may have different knowledge about these potential values, the prior knowledge is a form of subjective probability. For example, if you play cards, one person may have recorded the previous cards that were played, whereas other may not. They thus assign different probability of certain cards being played. In Bayesian inference, we consider \\(\\boldsymbol{\\theta}\\) as random variables to reflect our lack of knowledge about potential values taken. Italian scientist Bruno de Finetti, who is famous for the claim ``Probability does not exist’’, stated in the preface of Finetti (1974):\n\nProbabilistic reasoning — always to be understood as subjective — merely stems from our being uncertain about something. It makes no difference whether the uncertainty relates to an unforseeable future, or to an unnoticed past, or to a past doubtfully reported or forgotten: it may even relate to something more or less knowable (by means of a computation, a logical deduction, etc.) but for which we are not willing or able tho make the effort; and so on […] The only relevant thing is uncertainty — the extent of our knowledge and ignorance. The actual fact of whether or not the events considered are in some sense determined, or known by other people, and so on, is of no consequence.\n\nOn page 3, de Finetti continues (Finetti 1974)\n\nonly subjective probabilities exist — i.e., the degree of belief in the occurrence of an event attributed by a given person at a given instant and with a given set of information.\n\nThe likelihood \\(\\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{y}) \\equiv p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\\) is the starting point for Bayesian inference. However, we adjoin to it a prior distribution \\(p(\\boldsymbol{\\theta})\\) that reflects the prior knowledge about potential values taken by the \\(p\\)-dimensional parameter vector, before observing the data \\(\\boldsymbol{y}\\). We thus seek \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\): the observations are random variables but inference is performed conditional on the observed sample. By Bayes’ theorem, the posterior distribution \\(p(\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y})\\) is\n\\[\np(\\boldsymbol{\\Theta} \\mid \\boldsymbol{Y}) = \\frac{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\Theta}) p(\\boldsymbol{\\Theta})}{\\int p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}},\n\\tag{1.1}\\]\nso the posterior \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y})\\) is proportional, as a function of \\(\\theta,\\) to the product of the likelihood and the prior function. The integral in the denominator, termed marginal likelihood and denoted \\(p(\\boldsymbol{Y}) = \\mathsf{E}_{\\boldsymbol{\\theta}}\\{p(\\boldsymbol{Y} \\mid \\boldsymbol{\\theta})\\}\\), is a normalizing constant that makes the right hand side integrate to unity.\nFor the posterior to be proper, we need the product on the right hand side to be integrable. The denominator of Equation 1.1 is a normalizing constant so that the posterior is a distribution. If \\(\\boldsymbol{\\theta}\\) is low dimensional, numerical integration such as quadrature methods can be used to compute the latter. To obtain the marginal posterior, \\[p(\\theta_j \\mid \\boldsymbol{y}) = \\int p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}) \\mathrm{d} \\boldsymbol{\\theta}_{-j},\\] additional integration is needed.\n\nConsider a binomial likelihood with probability of success \\(p\\) and \\(n\\) trials, \\(Y \\sim \\mathsf{Bin}(n, p)\\). If we take a beta prior, \\(p \\sim \\mathsf{Be}(\\alpha, \\beta)\\) and observe \\(k\\) successes, the posterior is \\[\\begin{align*}\np(\\theta \\mid y = k) &\\propto \\binom{n}{k} p^k (1-p)^{n-k} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}p^{\\alpha-1} (1-p)^{\\beta-1}\n\\\\&\\stackrel{p}{\\propto} p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}\n\\end{align*}\\] and is \\[\\int_{0}^{1} p^{k+\\alpha-1}(1-p)^{n-k+\\beta-1}\\mathrm{d} p = \\frac{\\Gamma(k+\\alpha)\\Gamma(n-k+\\beta)}{\\Gamma(n+\\alpha+\\beta)},\\] a Beta function. Since we need only to keep track of the terms that are function of the parameter \\(p\\), we could recognize directly that the posterior distribution is \\(\\mathsf{Be}(k+\\alpha, n-k+\\beta)\\) and deduce the normalizing constant from there.\nThe number of success should be roughly \\(np\\) and the number of failures \\(n(1-p)\\) and so the likelihood contribution, relative to the prior, will dominate as the sample size \\(n\\) grows.\nAnother way to see this is to track moments (expectation, variance, etc.) The Beta distribution, whose density is \\(f(x; \\alpha, \\beta) \\propto x^{\\alpha-1} (1-x)^{\\beta-1}\\), has expectation \\(\\alpha/(\\alpha+\\beta)\\) and variance \\(\\alpha\\beta/\\{(\\alpha+\\beta)^2(\\alpha+\\beta+1)\\}\\) and so as \\(n \\to \\infty\\), the mean of the posterior will be dominated by information from the likelihood. An alternative parametrization takes \\(\\alpha=\\mu \\kappa\\), \\(\\beta = (1-\\mu)\\kappa\\) for \\(\\mu \\in (0,1)\\) and \\(\\kappa&gt;0\\), so that the model is parametrized directly in terms of mean \\(\\mu\\).\n\n\n\n\n\nFigure 1.1: Binomial likelihood for six successes out of 14 trials, \\(\\mathsf{Beta}(3/2, 3/2)\\) prior and posterior distribution from a beta-binomial model. The posterior curve is much closer to the likelihood than it is to the prior, even with a relatively small sample size.\n\n\n\n\n\n\nWhile a density integrates to 1 over the range of possible outcomes, the likelihood function does not when we integrate over the range of the parameters.\n\n\nThe beta-binomial model is an example of conjugate model, meaning the posterior distribution is from the same family as the prior.1 While we could calculate analytically the value of the normalizing constant, we could also for arbitrary priors use numerical integration in the event the parameter vector \\(\\boldsymbol{\\theta}\\) is low-dimensional. For a single scalar \\(p\\) on the unit interval, numerical integration or Monte Carlo integration yield nearly identical results.\n\nk &lt;- 6L # number of successes \nn &lt;- 14L # number of trials\nalpha &lt;- beta &lt;- 1.5 # prior parameters\nunnormalized_posterior &lt;- function(p){\n  p^(k+alpha-1) * (1-p)^(n-k + beta - 1)\n}\nintegrate(f = unnormalized_posterior,\n          lower = 0,\n          upper = 1)\n\n1.066906e-05 with absolute error &lt; 1e-12\n\n# Compare with known constant\nbeta(k + alpha, n - k + beta)\n\n[1] 1.066906e-05\n\n# Monte Carlo integration\nmean(unnormalized_posterior(runif(1e5)))\n\n[1] 1.064055e-05\n\n# Alternative approach, sampling from the prior\n# This is less efficient\nmean(dbinom(x = k, \n            size = n, \n            prob = rbeta(n = 1e6, alpha, beta))) *\n  beta(alpha, beta) / choose(n, k)\n\n[1] 1.065653e-05\n\n\n\nWhen \\(\\boldsymbol{\\theta}\\) is high-dimensional, the marginal likelihood is untractable. This is one of the main challenges of Bayesian statistics and the popularity and applicability has grown drastically with the development and popularity of numerical algorithms, following th publication of Geman and Geman (1984) and Gelfand and Smith (1990). Markov chain Monte Carlo methods circumvent the calculation of the denominator by drawing approximate samples from the posterior.\n\n1.2.1 Bayesian updating\nSubjective probabilities imply that different people with different prior beliefs would arrive at different conclusions. However, as more data are gathered, we can use Bayes theorem to update these prior beliefs and update the posterior. In most instances, the relative weight of the prior relative to the likelihood becomes negligible: if we consider independent data \\(\\boldsymbol{y}_1, \\boldsymbol{y}_n\\) observed sequentially, then \\[\\begin{align*}\np(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_k) &\\stackrel{\\boldsymbol{\\theta}}{\\propto} p(\\boldsymbol{y}_k \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\ldots, \\boldsymbol{y}_{k-1})\n\\\\ & \\stackrel{\\boldsymbol{\\theta}}{\\propto} \\prod_{i=1}^k p(\\boldsymbol{y}_i \\mid \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})\n\\end{align*}\\] If data are exchangeable, the order in which observations are collected and the order of the belief updating is irrelevant to the full posterior \\(p(\\boldsymbol{\\theta} \\mid \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n)\\).\n\n\n\n\nFinetti, Bruno de. 1974. Theory of Probability: A Critical Introductory Treatment. Vol. 1. New York: Wiley.\n\n\nGelfand, Alan E., and Adrian F. M. Smith. 1990. “Sampling-Based Approaches to Calculating Marginal Densities.” Journal of the American Statistical Association 85 (410): 398–409. https://doi.org/10.1080/01621459.1990.10476213.\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.” IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-6 (6): 721–41. https://doi.org/10.1109/TPAMI.1984.4767596.\n\n\nJegerlehner, Sabrina, Franziska Suter-Riniker, Philipp Jent, Pascal Bittel, and Michael Nagler. 2021. “Diagnostic Accuracy of a SARS-CoV-2 Rapid Antigen Test in Real-Life Clinical Settings.” International Journal of Infectious Diseases 109 (August): 118–22. https://doi.org/10.1016/j.ijid.2021.07.010.\n\n\nMathieu, Edouard, Hannah Ritchie, Lucas Rodés-Guirao, Cameron Appel, Charlie Giattino, Joe Hasell, Bobbie Macdonald, et al. 2020. “Coronavirus Pandemic (COVID-19).” Our World in Data."
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a property of exponential families that will be revisited in the next chapter.↩︎"
  },
  {
    "objectID": "priors.html#conjugate-priors",
    "href": "priors.html#conjugate-priors",
    "title": "2  Priors",
    "section": "2.1 Conjugate priors",
    "text": "2.1 Conjugate priors\nA distribution belongs to an exponential family with parameter vector \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^D\\) if it can be written as \\[\\begin{align*}\nf(y; \\boldsymbol{\\theta}) = \\exp\\left\\{ \\sum_{k=1}^K Q_k(\\boldsymbol{\\theta}) t_k(y) + D(\\boldsymbol{\\theta})\\right\\}\n\\end{align*}\\] and in particular, the support does not depend on unknown parameters. If we have an independent and identically distributed sample of observations \\(y_1, \\ldots, y_n\\), the log likelihood is thus of the form \\[\\begin{align*}\n\\ell(\\boldsymbol{\\theta}) = \\sum_{k=1}^K \\phi_k(\\boldsymbol{\\theta}) \\sum_{i=1}^n t_k(y_i) + n D(\\boldsymbol{\\theta}),\n\\end{align*}\\] where the collection \\(\\sum_{i=1}^n t_k(y_i)\\) (\\(k=1, \\ldots, K\\)) are sufficient statistics and \\(\\phi_k(\\boldsymbol{\\theta})\\) are the canonical parameters. The number of sufficient statistics are the same regardless of the sample size. Exponential families play a prominent role in generalized linear models, in which the natural parameters are modelled as linear function of explanatories.\nA log prior density that is proportional to \\[\\begin{align*}\n\\log p(\\boldsymbol{\\theta}) \\propto \\eta D(\\boldsymbol{\\theta}) + \\sum_{k=1}^K Q_k(\\boldsymbol{\\theta}) \\nu_k\n\\end{align*}\\] is conjugate.\n\nExample 2.1 (Conjugate priors for the binomial model) The binomial log density with \\(y\\) successes out of \\(n\\) trials is proportional to \\[\\begin{align*}\ny \\log(p) + (n-y) \\log(1-p) = y\\log\\left( \\frac{p}{1-p}\\right) + n \\log(1-p)\n\\end{align*}\\] with canonical parameter \\(\\mathrm{logit}(p)\\), which is the natural link function for Bernoulli, giving rise to logistic regresion model.\nSince the density of the binomial is of the form \\(p^y(1-p)^{n-y}\\), the beta distribution \\(\\mathsf{Be}(\\alpha, \\beta)\\) with density \\(f(x) \\propto x^{\\alpha-1} (1-x)^{\\beta-1}\\) is the conjugate prior.\nThe posterior mean \\[\\begin{align*}\n\\mathsf{E}(p \\mid y) = w\\frac{y}{n} + (1-w) \\frac{a}{a+b},\n\\qquad w = \\frac{n}{n+a+b}\n\\end{align*}\\] is therefore a weighted average of the maximum likelihood estimator and the prior mean. We can think of the parameter \\(\\alpha\\) (respectively \\(\\beta\\)) as representing the prior number of success (resp. failures).\n\n\nExample 2.2 (Conjugate prior for the Poisson model) The Poisson distribution with mean \\(\\mu\\) has log density proportional to \\(f(y; \\mu) \\propto y\\log(\\mu) -\\mu\\), so is an exponential family with natural parameter \\(\\log(\\mu)\\). The gamma distribution, \\(p(x) \\propto \\beta^{\\alpha}/\\Gamma(\\alpha)x^{\\alpha-1} \\exp(-\\beta x)\\) with shape \\(\\alpha\\) and rate \\(\\beta\\) is the conjugate prior for the Poisson. For an \\(n\\)-sample of independent observations \\(\\mathsf{Po}(\\mu)\\) observations with \\(\\mu \\sim \\mathsf{Ga}(\\alpha, \\beta)\\), the posterior is \\(\\mathsf{Ga}(\\sum_{i=1}^n y_i + \\alpha, \\beta + n)\\).\n\n\nExample 2.3 (Posterior rates for A/B tests using conjugate Poisson model) Upworthy.com, a US media publisher, revolutionized headlines online advertisement by running systematic A/B tests to compare the different wording of headlines, placement and image and what catches attention the most. The Upworthy Research Archive (Matias et al. 2021) contains results for 22743 experiments, with a click through rate of 1.58% on average and a standard deviation of 1.23%. The clickability_test_id gives the unique identifier of the experiment, clicks the number of conversion out of impressions. See Section 8.5 of Alexander (2023) for more details about A/B testing and background information.\nConsider an A/B test from November 23st, 2014, that compared four different headlines for a story on Sesame Street workshop with interviews of children whose parents were in jail and visiting them in prisons. The headlines tested were:\n\n\nSome Don’t Like It When He Sees His Mom. But To Him? Pure Joy. Why Keep Her From Him?\nThey’re Not In Danger. They’re Right. See True Compassion From The Children Of The Incarcerated.\nKids Have No Place In Jail … But In This Case, They Totally Deserve It.\nGoing To Jail Should Be The Worst Part Of Their Life. It’s So Not. Not At All.\n\n\nAt first glance, the first and third headlines seem likely to lead to a curiosity gap. The wording of the second is more explicit (and searchable), whereas the first is worded as a question.\nWe model the conversion rate \\(\\lambda_i\\) for each headline separately using a Poisson distribution and compare the posterior distributions for all four choices. Using a conjugate prior and selecting the parameters by moment matching yields approximately \\(\\alpha = 1.64\\) and \\(\\beta = 0.01\\) for the hyperparameters.\n\n\n\n\nTable 2.1: Number of views, clicks for different headlines for the Upworthy data.\n\n\nheadline\nimpressions\nclicks\n\n\n\n\nH1\n3060\n49\n\n\nH2\n2982\n20\n\n\nH3\n3112\n31\n\n\nH4\n3083\n9\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Gamma posterior for the Upworthy Sesame street headline.\n\n\n\n\nWe can visualize the posterior distributions. In this context, the large sample size lead to the dominance of the likelihood contribution \\(p(Y_i \\mid \\lambda_i) \\sim \\mathsf{Po}(n_i\\lambda_i)\\) relative to the prior. We can see there is virtually no overlap between different rates for headers H1 (preferred) relative to H4 (least favorable). The probability that Headline 3 is better than Headline 1 can be approximated by simulating samples from both posterior and computing the proportion of times one is larger: the probability of superiority is 1.7%, indicating a clear preference for the first headline H1.\n\n\nExample 2.4 (Conjugate priors in the Bayesian linear model) Consider a linear regression model with observation-specific mean \\(\\mu_i = \\mathbf{x}_i\\boldsymbol{\\beta}\\) \\((i=1,\\ldots, n)\\) with \\(\\mathbf{x}_i\\) the \\(i\\)th row of the \\(n \\times p\\) design matrix \\(\\mathbf{X}\\).\nConcatenating records, \\(\\boldsymbol{Y} \\sim \\mathsf{No}_n(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2 \\mathbf{Q}_y^{-1})\\), for a known precision matrix \\(\\mathbf{Q}_y\\), typically \\(\\mathbf{I}_n\\). To construct a conjugate joint prior for \\(p(\\boldsymbol{\\beta}, \\sigma^2)\\), we consider the sequential formulation \\[\\begin{align*}\n\\boldsymbol{\\beta} \\mid \\sigma^2 \\sim \\mathsf{No}_p(\\boldsymbol{\\nu}_\\beta, \\sigma^2 \\mathbf{Q}^{-1}_\\beta), \\qquad \\sigma^2 \\sim \\mathsf{IG}(\\alpha,\\beta)\n\\end{align*}\\] where \\(\\mathsf{IG}\\) denotes the inverse gamma distribution1\n\nThe joint posterior is Gaussian-inverse gamma and can be factorized \\[\\begin{align*}\np(\\boldsymbol{\\beta}, \\sigma^2 \\mid y) = p(\\sigma^2 \\mid y) p(\\boldsymbol{\\beta} \\mid \\sigma^2, y)\n\\end{align*}\\] where \\(p(\\sigma^2 \\mid y) \\sim \\mathsf{IG}(\\alpha^*, \\beta^*)\\) and \\(p(\\boldsymbol{\\beta} \\mid \\sigma^2, y) \\sim \\mathsf{No}_p(\\mathbf{M}\\boldsymbol{m}, \\sigma^2\\mathbf{M})\\) with \\(\\alpha^* = \\alpha + n/2\\), \\(\\beta^*=\\beta + 0.5 \\boldsymbol{\\nu}_\\beta^\\top \\mathbf{Q}_\\beta\\boldsymbol{\\nu}_\\beta + \\boldsymbol{y}^\\top\\boldsymbol{y} - \\boldsymbol{m}^\\top\\mathbf{M}\\boldsymbol{m}\\), \\(\\boldsymbol{m} = \\mathbf{Q}_\\beta \\boldsymbol{\\nu}_\\beta + \\mathbf{X}^\\top \\mathbf{Q}_y\\boldsymbol{y}\\) and \\(\\mathbf{M} = (\\mathbf{Q}_\\beta + \\mathbf{X}^\\top\\mathbf{Q}_y\\mathbf{X})^{-1}\\); the latter can be evaluated efficiently using Shermann–Morrisson–Woodbury identity.\n\nThe exponential family is quite large; Fink (1997) A Compendium of Conjugate Priors gives multiple examples of conjugate priors and work out parameter values.\n\nOne criticism of the Bayesian approach is the arbitrariness of prior functions. However, the role of the prior is often negligible in large samples (consider for example the posterior of exponential families with conjugate priors). Moreover, the likelihood is also chosen for convenience, and arguably has a bigger influence on the conclusion. Data fitted using a linear regression model seldom follow Gaussian distributions conditionally, in the same way that the linearity is a convenience (and first order approximation).\n\nIn general, unless the sample size is small and we want to add expert opinion, we may wish to pick an uninformative prior, i.e., one that does not impact much the outcome. For conjugate models, one can often show that the relative weight of prior parameters (relative to the random sample likelihood contribution) becomes negligible by investigating their relative weights."
  },
  {
    "objectID": "priors.html#uninformative-priors",
    "href": "priors.html#uninformative-priors",
    "title": "2  Priors",
    "section": "2.2 Uninformative priors",
    "text": "2.2 Uninformative priors\n\nDefinition 2.1 (Proper prior) We call a prior proper if it’s integral is finite; such prior function automatically leads to a valid posterior.\n\nThe best example of prior priors arise from probability density function. We can still employ this rule for improper priors: for example, taking \\(\\alpha, \\beta \\to 0\\) in the beta prior leads to a prior proportional to \\(x^{-1}(1-x)^{-1}\\), the integral of which diverges on the unit interval \\([0,1]\\). However, as long as the number of success and the number of failures is larger than 1, meaning \\(k \\geq 1, n-k \\geq 1\\), the posterior distribution would be proper, i.e., integrable. To find the posterior, normalizing constants are also superfluous.\nMany uninformative priors are flat, or proportional to a uniform on some subset of the real line and therefore improper. It may be superficially tempting to set a uniform prior on a large range to ensure posterior property, but the major problem is that a flat prior may be informative in a different parametrization, as the following example suggests.\n\nExample 2.5 Consider the parameter \\(\\log(\\tau) \\in \\mathbb{R}\\) and the prior \\(p( \\log \\tau) \\propto 1\\). If we reparametrize the model in terms of \\(\\tau\\), the new prior (including the Jacobian of the transformation) is \\(\\tau^{-1}\\)\n\nSome priors are standard and widely used. In location scale families with location \\(\\nu\\) and scale \\(\\tau\\), the density is such that \\[\\begin{align*}\nf(x; \\nu, \\tau) =  \\frac{1}{\\tau} f\\left(\\frac{x - \\nu}{\\tau}\\right), \\qquad \\nu \\in \\mathbb{R}, \\tau &gt;0.\n\\end{align*}\\] We thus wish to have a prior so that \\(p(\\tau) = c^{-1}p(\\tau/c)\\) for any scaling \\(c&gt;0\\), whence it follows that \\(p(\\tau) \\propto \\tau^{-1}\\), which is uniform on the log scale.\nThe priors \\(p(\\nu) \\propto 1\\) and \\(p(\\tau) \\propto \\tau^{-1}\\) are both improper but lead to location and scale invariance, hence that the result is the same regardless of the units of measurement.\n\nDefinition 2.2 (Jeffrey’s prior) In single parameter models, taking a prior function for \\(\\theta\\) proportional to the square root of the determinant of the information matrix \\(p(\\theta) \\propto \\imath(\\theta)\\) yields a prior that is invariant to parametrization, so that inferences conducted in different parametrizations are equivalent.\n\nTo see this, consider a bijective transformation \\(\\theta \\mapsto \\vartheta\\). Under the reparametrized model and suitable regularity conditions2, the chain rule implies that \\[\\begin{align*}\ni(\\vartheta) &= - \\mathsf{E} \\left(\\frac{\\partial^2 \\ell(\\vartheta)}{\\partial^2 \\vartheta}\\right)\n\\\\&= - \\mathsf{E}\\left(\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2}\\right) \\left( \\frac{\\mathrm{d} \\theta}{\\mathrm{d} \\vartheta} \\right)^2 + \\mathsf{E}\\left(\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}\\right) \\frac{\\mathrm{d}^2 \\theta}{\\mathrm{d} \\vartheta^2}\n\\end{align*}\\] Since the score has mean zero, \\(\\mathsf{E}\\left\\{\\partial \\ell(\\theta)/\\partial \\theta\\right\\}=0\\), the rightmost term vanishes. We can thus relate the Fisher information in both parametrizations, with \\[\\begin{align*}\n\\imath^{1/2}(\\vartheta) = \\imath^{1/2}(\\theta) \\left| \\frac{\\mathrm{d} \\theta}{\\mathrm{d} \\vartheta} \\right|,\n\\end{align*}\\] implying invariance.\nMost of the times, Jeffrey’s prior is improper. For the binomial model, it can be viewed as a limiting conjugate beta prior with \\(\\alpha, \\beta\\to 0\\)). Unfortunately, in multiparameter models, the system isn’t invariant to reparametrization if we consider the determinant of the Fisher information.\n\n\nExample 2.6 Consider the binomial distribution \\(f(y; \\theta, n) \\propto \\theta^y(1-\\theta)^{n-y}\\mathsf{I}_{\\theta \\in [0,1]}\\). The negative of the second derivative of the log likelihood with respect to \\(p\\) is \\(\\jmath(\\theta) = - \\partial^2 \\ell(\\theta; y) / \\partial \\theta^2 = y/\\theta^2 + (1-y)/(1-\\theta)^2\\) and since \\(\\mathsf{E}(Y)=n\\theta\\), thus the Fisher information is \\(\\imath = \\mathsf{E}\\{\\jmath(\\theta)\\}=n/\\theta + n/(1-\\theta) = n/\\{\\theta(1-\\theta)\\}\\).3\nJeffrey’s prior is thus \\(p(\\theta) \\propto \\theta^{-1}(1-\\theta)^{-1}\\).\n\n\nCheck that for the Gaussian distribution \\(\\mathsf{No}(\\mu, \\sigma^2)\\), the Jeffrey’s prior obtained by treating each parameter in turn, fixing the value of the other, are \\(p(\\mu) \\propto 1\\) and \\(p(\\sigma) \\propto 1/\\sigma\\), which also correspond to the default uninformative priors for location-scale families.\n\n\nExample 2.7 The Poisson distribution with \\(\\ell(\\lambda) \\propto -\\lambda + y\\log \\lambda\\), with second derivative \\(-\\partial^2 \\ell(\\lambda)/\\partial \\lambda^2 = y/\\lambda^2\\). Since the mean of the Poisson distribution is \\(\\lambda\\), the Fisher information is \\(\\imath(\\lambda) = \\lambda^{-1}\\) and Jeffrey’s prior is \\(\\lambda^{-1/2}\\)."
  },
  {
    "objectID": "priors.html#expert-knowledge",
    "href": "priors.html#expert-knowledge",
    "title": "2  Priors",
    "section": "2.3 Expert knowledge",
    "text": "2.3 Expert knowledge\nTh prior distribution may have parameters themselves that need to be specified by experts. One may also wish to add another layer and set an hyperprior distribution on the parameters, resulting in a hierarchical model.\nSetting parameters of priors is often done by reparametrizing the latter in terms of moments. Sometimes, it may be easier to set priors in a different scale where subject-matter expertise is most easily elicited.\n\nExample 2.8 The generalized extreme value distribution arises as the limiting distribution for the maximum of \\(m\\) independent observations. The \\(\\mathsf{GEV}(\\mu, \\sigma, \\xi)\\) distribution is a location-scale with distribution function \\[\\begin{align*}\nF(x) = \\exp\\left\\{ - \\left(1+\\xi(x-\\mu)/\\sigma\\right)^{-1/\\xi}_{+}\\right\\}\n\\end{align*}\\] where \\(x_{+} = \\max\\{0, x\\}\\).\nInverting the distribution function yields the quantile function \\[\\begin{align*}\nQ(p) \\mu + \\sigma \\frac{(-\\log p)^{-\\xi}-1}{\\xi}\n\\end{align*}\\]\nIn environmental data, we often model annual maximum. Engineering designs are often specified in terms of the \\(k\\)-year return levels, defined as the quantile of the annual maximum exceeded with probability \\(1/k\\) in any given year. Using a \\(\\mathsf{GEV}\\) for annual maximum, Coles and Tawn (1996) proposed modelling annual daily rainfall and specifying a prior on the quantile scale \\(q_1 &lt; q_2 &lt; q_3\\) for tail probabilities \\(p_1&gt; p_2 &gt; p_3\\). To deal with the ordering constraints, gamma priors are imposed on the differences \\(q_1 - o \\sim \\mathsf{Ga}(\\alpha_1, \\beta_1)\\), \\(q_2 - q_1 \\sim \\mathsf{Ga}(\\alpha_2, \\beta_2)\\) and \\(q_3-q_2 \\sim \\mathsf{Ga}(\\alpha_3, \\beta_3)\\), where \\(o\\) is the lower bound of the support. The prior is thus of the form\n\\[\\begin{align*}\np(\\boldsymbol{q}) \\propto q_1^{\\alpha_1-1}\\exp(-\\beta_1 q_1) \\prod_{i=2}^3 (q_i-q_{i-1}^{\\alpha_i-1} \\exp\\{\\beta_i(q_i-q_{i-1})\\}.\n\\end{align*}\\] where \\(0 \\leq q_1 \\leq q_2 \\leq q_3\\). We can then relate the prior parameters to moments.\nConsider the annual maximum rainfall in Abisko, Sweden.\n\n\nExample 2.9 (Prior simulation) Are the prior reasonable? One way to see this is to sample values from the priors and generate new observations.\n\n\nExample 2.10 We can specify gamma back-transform them to location \\(\\mu\\), scale \\(\\sigma\\) and shape \\(\\xi\\) and simulate observations from the \\(\\mathsf{GEV}(\\mu, \\sigma, \\xi)\\) and compare them to observations.\n\n\n\n\n\n\n\nAlexander, Rohan. 2023. Telling Stories with Data: With Applications in R. Boca Raton, FL: CRC Press.\n\n\nColes, Stuart G., and Jonathan A. Tawn. 1996. “A Bayesian Analysis of Extreme Rainfall Data.” Journal of the Royal Statistical Society. Series C (Applied Statistics) 45 (4): 463–78. https://doi.org/10.2307/2986068.\n\n\nMatias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole. 2021. “The Upworthy Research Archive, a Time Series of 32,487 Experiments in U.S. Media.” Scientific Data 8 (195). https://doi.org/10.1038/s41597-021-00934-7."
  },
  {
    "objectID": "priors.html#footnotes",
    "href": "priors.html#footnotes",
    "title": "2  Priors",
    "section": "",
    "text": "This simply means that the precision \\(\\sigma^{-2}\\), the reciprocal of the variance, has a gamma distribution with shape \\(\\alpha\\) and rate \\(\\beta\\).↩︎\nUsing Bartlett’s identity; Fisher consistency can be established using the dominated convergence theorem.↩︎\nThe Fisher information is linear in the sample size for independent and identically distributed data.↩︎"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Alexander, Rohan. 2023. Telling Stories with Data: With Applications\nin R. Boca Raton, FL: CRC Press.\n\n\nColes, Stuart G., and Jonathan A. Tawn. 1996. “A\nBayesian Analysis of Extreme Rainfall Data.”\nJournal of the Royal Statistical Society. Series C (Applied\nStatistics) 45 (4): 463–78. https://doi.org/10.2307/2986068.\n\n\nFinetti, Bruno de. 1974. Theory of Probability: A Critical\nIntroductory Treatment. Vol. 1. New York: Wiley.\n\n\nGelfand, Alan E., and Adrian F. M. Smith. 1990. “Sampling-Based\nApproaches to Calculating Marginal Densities.” Journal of the\nAmerican Statistical Association 85 (410): 398–409. https://doi.org/10.1080/01621459.1990.10476213.\n\n\nGeman, Stuart, and Donald Geman. 1984. “Stochastic Relaxation,\nGibbs Distributions, and the Bayesian\nRestoration of Images.” IEEE Transactions on Pattern Analysis\nand Machine Intelligence PAMI-6 (6): 721–41. https://doi.org/10.1109/TPAMI.1984.4767596.\n\n\nJegerlehner, Sabrina, Franziska Suter-Riniker, Philipp Jent, Pascal\nBittel, and Michael Nagler. 2021. “Diagnostic Accuracy of a\nSARS-CoV-2 Rapid Antigen Test in Real-Life Clinical\nSettings.” International Journal of Infectious Diseases\n109 (August): 118–22. https://doi.org/10.1016/j.ijid.2021.07.010.\n\n\nMathieu, Edouard, Hannah Ritchie, Lucas Rodés-Guirao, Cameron Appel,\nCharlie Giattino, Joe Hasell, Bobbie Macdonald, et al. 2020.\n“Coronavirus Pandemic (COVID-19).” Our World in\nData.\n\n\nMatias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles\nEbersole. 2021. “The Upworthy Research\nArchive, a Time Series of 32,487 Experiments in\nU.S. Media.” Scientific Data 8 (195). https://doi.org/10.1038/s41597-021-00934-7."
  }
]